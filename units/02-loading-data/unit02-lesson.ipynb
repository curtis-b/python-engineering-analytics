{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 2: Loading Engineering Data\n",
    "----------------------------------\n",
    "\n",
    "Once we understand the basics of working with Python in a Jupyter Notebook, our most fundamental task is to load a set of data which we will then manipulate and analyze. Engineering data may come in different formats, such as text files, spreadsheets or SQL databases. Python packages are available to easily import data from these sources. In future units, we will build upon this foundation and analyze the data that we have loaded using these techniques.\n",
    "\n",
    "**After completing this unit, you should be able to:**\n",
    "\n",
    "- Understand when to select the `numpy` or `pandas` packages for loading data\n",
    "- Load numeric data into a `numpy.ndarray` object\n",
    "- Load mixed-format data into a `pandas.DataFrame` object\n",
    "- Calculate basic statistics for a dataset\n",
    "- Save data to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. The `numpy` package for processing numerical data\n",
    "\n",
    "In Unit 1 we discussed the Python ecosystem, including the fact that there are many add-on packages to extend the basic functionality of Python. One fundamental package for engineering analysis is [`numpy`](https://numpy.org/doc/stable/) (*Num*erical *Py*thon). Specifically, this package defines an object called the `ndarray` (n-dimensional array), which is used for vector and matrix calculations. As we'll see, this data structure looks similar to the list object that we learned previously but it is far superior for data analysis. The `numpy` package includes functions for creating, loading, modifying and saving data in the `ndarray` object. The functions in this package are written in the C programming language, and take advantage of CPU parallelism to offer high performance (computing speed). Luckily, you don't need to know how this works to benefit from it. \n",
    "\n",
    "To use the `numpy` package, you will add the following `import` line to a code cell at the top of your notebook. Note that we are renaming (or, if you prefer, creating an alias for) the `numpy` package as `np` in this instruction. You will commonly find this type of shorthand used. \n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "Now, when we want to reference `numpy`, we'll instead type `np`. The popular Python packages have a typical abbreviation that you will see referenced in literature. Technically, you could create any alias (using our variable naming rules): `curts_favorite_package`, `package1`, `a`, ... but you'll find that `np` is the standard for `numpy`.\n",
    "\n",
    "### 2.1.1. Creating a 1-dimensional `ndarray`\n",
    "\n",
    "There are several ways that we can create an array of values from scratch:\n",
    "\n",
    "1. Passing a Python `list` containing the values\n",
    "2. Generating a sequence (range) of values based on a starting point and step interval (useful for plotting, or evaluating a function at different points)\n",
    "3. Generating $n$ values evenly distributed between a start and end point (also useful for plotting, or evaluating a function at different points)\n",
    "4. Generating $n$ random values\n",
    "\n",
    "An example of each method is provided in the code cell below. \n",
    "\n",
    "Using a Python `list` is pretty straightforward: simply pass the list (or a variable name that references a list) to the `np.array(list)` function. In `numpy`, all of the elements of an array must be of the same type so you cannot build an `ndarray` from a list that contains mixed data types.\n",
    "\n",
    "The *array range* or `np.arange(start, stop, step)` allows us to generate an array as an arithmetic sequence. We define the start, stop and step values as parameters in the function. The start value will be the first element in the array. The step value is the number by which we count to generate the sequence (this can be real positive or negative number). The sequence will continue **up to, but not including** the stop value.\n",
    "\n",
    "There are two related functions, `np.linspace(start, stop, n)` and `np.logspace(start, stop, n)`, that allow us to generate $n$ elements between a start and stop value. Unlike the `np.arange()` function, both the start and stop values **are included** in the array. As the names implies, `np.linspace()` will distribute the values linearly and `np.logspace()` will distribute values logarithmically. When using `np.logspace()` the start and stop are provided in terms of the power of 10, forming a range [$10^{start}, 10^{stop}$]. This can be helpful when evaluating a function that will be plotted on a log scale.\n",
    "\n",
    "Finally, we can use the `np.random` module to generate random numbers. This module contains a function `np.random.random(n)` that generates a 1 $\\times$ $n$ array of random numbers from the interval [0, 1). Note that the duplication of the word *random* is not a typo. This code calls the `random(n)` function from the `np.random` module, which can admittedly be a bit confusing. If you need to sample from other probability distributions, or generate random integers, there are other functions available in [`np.random`](https://numpy.org/doc/stable/reference/random/index.html) that you can look up and use. We'll also explore sampling random values from statistical distributions in Unit 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create an array from a list\n",
    "# the parameter is a list of values (or a variable name that represents a list)\n",
    "arr_from_list = np.array([1, 2, 3])\n",
    "print(f'array_from_list: {arr_from_list}')\n",
    "\n",
    "# create an array as a sequence of values\n",
    "# the parameters are the starting value, the stop value (which is not included), \n",
    "# and the step value\n",
    "arr_range = np.arange(1, 11, 1)\n",
    "print(f'arr_range: {arr_range}')\n",
    "\n",
    "# create an array of 5 values, logarithmically distributed between 1 (10^0) \n",
    "# and 100 (10^2)\n",
    "arr_logspace = np.logspace(0, 2, 5)\n",
    "print(f'arr_logspace: {arr_logspace}')\n",
    "\n",
    "# create an array of random values between 0, 1\n",
    "# the parameter is the number of values\n",
    "arr_random = np.random.random(6)\n",
    "print(f'arr_random: {arr_random}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Manipulating the data in an `ndarray`\n",
    "\n",
    "Indexing and slicing the `ndarray` works just like `list`, so you can use the same techniques that we learned in Unit 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the array uses the same indexing/slicing syntax as a Python list\n",
    "arr_range_reversed = arr_range[::-1]\n",
    "print(f'Reversed Array: {arr_range_reversed}')\n",
    "\n",
    "arr_range_topfive = arr_range[:5]\n",
    "print(f'First 5 Items: {arr_range_topfive}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike a standard Python `list`, we can apply math operators directly to an `ndarray`. This will apply the operation *element-wise* (to each item in the array individually). For example, to square each of the values in the array, we just square the array object. The `numpy` code takes care of the loop for us behind the scenes, so a list comprehension is not required. Because of how the `numpy` library is programmed, this will be much faster on large datasets compared to the list comprehension that we used in Unit 1. The return value is a new `ndarray`, which can be saved to a variable name. The existing array is not modified by this operation, but we can reuse the same name (to replace the original values) if we choose to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_range**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually measure the performance improvement of using a `numpy.ndarray` versus a Python `list` to compute the square of a set of numbers. The code block below will measure the time to compute the square (element-wise) for arrays and lists of increasing length. When the results are generated, the code will create a plot of the results for you to see. We'll learn how the plotting code works in Unit 3. This process of testing how execution time increases with increasingly large datasets is a *scaling analysis*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# an array containing the number of elements, n, in the array\n",
    "n_arr = 10**np.arange(3, 7, 1)\n",
    "\n",
    "# empty arrays to hold the times that we measure for the math operation\n",
    "time_list = np.zeros(len(n_arr))\n",
    "time_ndarray = np.zeros(len(n_arr))\n",
    "\n",
    "# measure the time to calculate hte square of each element in the array\n",
    "# the loop steps through each array length that we defined above\n",
    "for i, n in enumerate(n_arr):\n",
    "\n",
    "    # generate an array of random numbers to test\n",
    "    random_ndarray = np.random.random(n)\n",
    "\n",
    "    # convert the array to a list for the list comprehension test\n",
    "    random_list = random_ndarray.tolist()\n",
    "\n",
    "    # measure the time for the ndarray\n",
    "    time_start = perf_counter()\n",
    "    squared_ndarray = random_ndarray**2\n",
    "    time_end = perf_counter()\n",
    "    time_ndarray[i] = time_end - time_start\n",
    "\n",
    "    # measure the time for the ndarray\n",
    "    time_start = perf_counter()\n",
    "    squared_list = [x**2 for x in random_list]\n",
    "    time_end = perf_counter()\n",
    "    time_list[i] = time_end - time_start\n",
    "\n",
    "# display the results\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(n_arr, time_ndarray, label='numpy.ndarray')\n",
    "ax.plot(n_arr, time_list, label='list comprehension')\n",
    "\n",
    "ax.set_xlabel('Array/list Length (n)')\n",
    "ax.set_ylabel('Execution Time (s)')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax.semilogx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small datasets ($n<10^4$), we don't see a big difference in performance. However, as array size gets large, we see the benefits of the optimization (vectorization and parallel processing) built into the `numpy` package.\n",
    "\n",
    "When executing other mathematical operations on an array, we will use versions of the math functions that are built-in to the `numpy` package -- not the basic `math` module functions. These functions are designed to process the `ndarray` efficiently. In general, they have the same names as the functions from the `math` module. We preface the function name with `np.` to indicate that we want the function that is coming from `numpy`. In the example below, the `sin` function has a parameter which takes an `ndarray` object, and returns another `ndarray` with the result.\n",
    "\n",
    "Refer to the [Mathematical functions](https://numpy.org/doc/stable/reference/routines.math.html) section of the `numpy` documentation for more detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sin(arr_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ndarray` object also has methods that can be used to calculate basic statistics on the array values. These functions are called by writing the `ndarray` variable name, and then the function name, seperated by a period. No additional parameters are necessary, but we still must include the empty parentheses (`()`) to indicate that we are calling a function.\n",
    "\n",
    "```\n",
    "<variablename>.<functionname>()\n",
    "```\n",
    "\n",
    "Some examples of useful functions are given in the example below. Here, we use the *f-string* that we learned about in Unit 1. In addition to the variable name, we can also include these function calls within the braces `{}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Minimum Value: {arr_random.min():0.3f}')\n",
    "print(f'Maximum Value: {arr_random.max():0.3f}')\n",
    "print(f'Mean: {arr_random.mean():0.3f}')\n",
    "print(f'Standard Deviation: {arr_random.std():0.3f}')\n",
    "print(f'Sum: {arr_random.sum():0.3f}')\n",
    "print(f'Cumulative Sum: {arr_random.cumsum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Saving and Loading Files\n",
    "\n",
    "At some point, you'll need to save your results to a file for storage or to share with a colleague. For our purposes, we'll demonstrate the use of plain text files for storing data. These are files that you'll be able to open and read in a text viewer like Notepad, or import into a spreadsheet. You can also save data in a binary format that will take up less space on your hard drive. However, this won't be easily human-readable, so we'll skip it for now.\n",
    "\n",
    "To save our `ndarray` to a text file, use the [`np.savetxt()`](https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html) function. This requires you to provide a filename, and an `ndarray` as parameters. Optionally, you can apply formatting to the results, or change how columns in your array are separated. The default is a space (`' '`), but you may wish to set this to a comma (`','`) to create a comma-separated (csv) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../../output/unit2_arr_range.txt', arr_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter gives the path to the file that we want to create, including the new file name. If we just used a file name (no directory name) the file would be saved in the *current working directory*, which is the directory where this Jupyter Notebook resides. Often, it's desirable to separate code from data/output files in a project to reduce clutter, so we have chosen to use a different storage location for this output.\n",
    "\n",
    "For anyone familiar with working on the Windows or Linux command-line shell, the directory notation will be familiar. The double-period (`..`) means go up one level in the directory tree from where we are right now. \n",
    "\n",
    "- python-engineering-analytics\n",
    "  - assets\n",
    "  - data\n",
    "  - output\n",
    "  - units\n",
    "    - 01-introduction\n",
    "    - 02-loading-data\n",
    "      - **unit02-lesson.ipynb** <- we are here\n",
    "      - unit02-problems.ipynb\n",
    "      - unit02-solutions.ipynb\n",
    "    - ...\n",
    "\n",
    "From where we are now, in the `02-loading-data` directory, we go up one level (`../`) to the `units` directory. Then, we go up another level (`../`) to the `python-engineering-analytics` directory. Then we can go down into the `output/` directory and finally assign a file name of `unit2_arr_range.txt`. Chaining these together gives us the filename string from the example.\n",
    "\n",
    "After you have run this cell on your computer, you should see a new file appear in the output directory, called `unit2_arr_range.txt`. Open this file and you will see the array values, one per line. \n",
    "\n",
    "Loading a file is also a single line of code using `np.loadtxt()`. If we changed the formatting of the file that we saved (such as using a comma the delimiter character, we'll need to supply that parameter to the function also. Here, we reload the data that we previously saved into a new `ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_range_reload = np.loadtxt('../../output/unit2_arr_range.txt')\n",
    "arr_range_reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Multi-Dimensional Arrays\n",
    "\n",
    "So far, we have looked only at 1-dimensional arrays (vectors). However, as the name suggests, the `ndarray` object can also handle 2-dimensional matrices and even higher-order structures. The `../../data/` directory houses two csv files (comma-separated), `matrix_A.csv` and `matrix_B.csv`, that each contains a 2-dimensional 10 $\\times$ 10 matrix. We'll load those files into memory, and use them for the next examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.loadtxt('../../data/matrix_A.csv', delimiter=',')\n",
    "B = np.loadtxt('../../data/matrix_B.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can index or slice multi-dimensional arrays just like we did with 1-dimensional arrays. Inside of the square brackets (`[]`) we have an index or slice associated with each dimension. These are separated by commas.\n",
    "\n",
    "```\n",
    "<ndarraynamee>[<rowslice>, <columnslice>]\n",
    "```\n",
    "\n",
    "If you want to return all rows or columns, use the colon (`:`) in that position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the entire matrix\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the value at row=1, column=1 (remember that we start counting at 0)\n",
    "A[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the first 3 rows, all columns\n",
    "A[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view row 1, columns 4-6\n",
    "A[1, 4:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, multiple columns can be selected by using a `list` object for the slicer. In this example, we select all rows but only columns 2 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[:, [2, 5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy` includes a new operator for matrix multiplication, `@`, which performs row $\\times$ column multiplication on vectors and matrices. As you can see in the example below, matrices `A` and `B` are actually inverses of each other, because their matrix product is the idenity matrix.\n",
    "\n",
    "We'll use the `np.round()` function to perform an element-wise rounding (to 1 decimal place) on each of the values to clean up the output. This function takes an `ndarray` as its first parameter and returns a new, rounded, `ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication of A and B\n",
    "C = A@B\n",
    "np.round(C, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the matrix inverse is also possible using the function, `np.linalg.inv()`. Because $A^{-1}=B$, we should find that $A^{-1}-B=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.linalg.inv(A) - B, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can learn about the size of an `ndarray` by accessing its `size` or `shape` properties. These are not functions, so we do not write the parentheses (`()`) after the name.\n",
    "\n",
    "- `ndarray.size` provides the total number of elements in the array\n",
    "- `ndarray.shape` provides the length of the array in each dimension\n",
    "\n",
    "For a 1-dimensional array, such as `arr_random`, we see that the shape only has one value of `5`, but it is enclosed in parenthesis `()`. This is because the `shape` property has a type of `tuple`. We can think about a `tuple` as a special type of list, but the items in a `tuple` cannot be modified after it is created. The `ndarray.shape` property of the 2-dimensional matrix shows us that the row count and column count are each 10. The indexing of values in a `tuple` is identical to the indexing of a `list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_random.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_random.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes the computer stores the n-dimensional array as a single 1-dimensional array, and our ability to index by rows and columns is really just a convenience. We can ask the computer to change the shape, so that we can index the array differently. \n",
    "\n",
    "To manipulate the 1d vectors that we created earlier into 2d matrices we use the `ndarray.reshape()` function. The new shape is supplied as a either a `tuple` (enclosed by parentheses) or `list` (enclosed by square brackets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_random.reshape((3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_random.reshape((2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these examples, we see that `numpy` is reshaping the data in *row-major* order (C-style). This means that each element is filled into the first row, across the columns, before moving on to the next row. For another clear example, see below. Here, we create a 50-element sequence (using the default values of starting at 0 and counting by 1), and reshape that to have 10 rows and 5 columns. Notice that we can count across the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(50).reshape((10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can *transpose* the matrix (swapping the *row* and *column* indices for each element in the matrix) by accessing the `.T` attribute of the `ndarray`. In the cell below, we generate the same matrix that we looked at above and apply the matrix transpose to yield a 5 row $\\times$ 10 column matrix. Note how this is different from reshaping the array directly into a shape of $\\left(5, 10\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(50).reshape((10, 5)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. The `pandas` package for mixed-format data\n",
    "\n",
    "While `numpy` is extremely useful (and efficient) for numerical analysis and matrix math, sometimes it is more convenient to work with a dataset that has named columns, or one that contains a mix of datatypes (dates, strings, numbers) that are not supported in a single `ndarray`. This is where the [`pandas`](https://pandas.pydata.org/) package can be extremely useful. To import the `pandas` package, we need to add the following line of code:\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "Like the abbreviation `np` for `numpy`, it is customary to use the abbreviation of `pd` for `pandas`.\n",
    "\n",
    "In `pandas`, data is organized in an object called a `DataFrame`. Think about a `DataFrame` as a table in a spreadsheet, with named columns and a row index. The row index can be just the row number, or we can assign an index of a different data type. \n",
    "\n",
    "### 2.2.1. Loading data into a `pandas.DataFrame`\n",
    "\n",
    "While we can build a `DataFrame` from scratch, our mission is to analyze engineering data so we're most interested in loading data that already exists. There are three formats that we will load data from:\n",
    "\n",
    "1. A comma- or tab-delimited plain text file, such as a csv or txt\n",
    "2. An Excel spreadsheet file (xlsx)\n",
    "3. The result set from a SQL SELECT query\n",
    "\n",
    "Once the `DataFrame` is populated, it doesn't matter where the data came from. The process for using these sources is similar, but we'll provide an example of each. First, we'll look at a 2-column csv file using the `pd.read_csv()` function. The directory path is written in the same way as we learned above. Also, take a look at the source file and notice that the first row contains column names. This is important -- `pandas` will be expecting names for the columns in a `DataFrame` object.\n",
    "\n",
    "Once we read the file into a `DataFrame` object, it can be helpful to use the `DataFrame.head()` function to view the top $n$ rows (the default is 5). This provides the column names in the `DataFrame` and gives an indication of the type of data that is contained in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read a .csv file into a DataFrame variable named df\n",
    "df = pd.read_csv('../../data/pcr-polyethylene_gc-fid.csv')\n",
    "\n",
    "# display the top n rows in the DataFrame named df (defaults to n=5 if blank)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading a Microsoft Excel file is very similar, but uses a function called `pd.read_excel()`. In addition to the file path, there is a parameter `sheet_name` to identify the name of the worksheet in the file. Only one worksheet from the file can be read at a time, but there are functions in the `pandas` package available to join sheets together by appending data (`pd.concat()`) or joining based on a shared value (`pd.merge()`), similar to a \"vlookup\". In the example below, we see that this `DataFrame` contains a mix of text and numerical data that we will use in later units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a .xlsx file into a DataFrame variable named df\n",
    "df_film = pd.read_excel('../../data/film_testing.xlsx', sheet_name='physical_properties')\n",
    "\n",
    "# display the top n rows in the DataFrame named df (defaults to n=5 if blank)\n",
    "df_film.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your data may also be present in a database that you can query into a `DataFrame`. The langage for querying a relational database, called *structured query language* or **SQL** could be a course unto itself. In the UW-MEDA program, database design and querying data is taught as a semester-long course: LIS 751. What follows will simply introduce the capability. In a relational database, data is often divided into separate tables, which can be merged together by shared id columns. For this example, we break the data that was presented in an Excel file above into two separate tables: Measurements and Properties. The Properties table contains just the individual property values that we will be measuring, and an ID number. This ID number, referring to a property name, is stored in each record in the Measurements table. \n",
    "\n",
    "**MEASUREMENTS**\n",
    "- MeasurementID\n",
    "- FilmID\n",
    "- FilmType\n",
    "- PropertyID\n",
    "- Direction\n",
    "- Measurement\n",
    "\n",
    "**PROPERTIES**\n",
    "- PropertyID\n",
    "- PropName\n",
    "\n",
    "Storing the data in this way can help prevent some errors (such as typos in the property name). It can also reduce file size on the computer system, by repeating only the 4-byte integer value for the PropertyID in each row in the Measurements table, as opposed to the longer text string. The text value for the property name is only stored once, in the Properties table. \n",
    "\n",
    "This example uses the file-based database [SQLite](https://www.sqlite.org/index.html) so that we can easily package the example database with this repo. More commonly, you will find that databases operate as a server process (rather than a file), and that you will need to connect to the database using a *connection string* and a package like [SQLAlchemy](https://www.sqlalchemy.org/). Once the connection is established, the SQL query and `pandas` commands will be the same.\n",
    "\n",
    "While it may be helpful to know that Python can directly connect to a database, the use of SQL is outside of the scope of this course. If you are interested in learning more, you could try [W3 School's SQL Tutorial](https://www.w3schools.com/sql/) or a book like [Head First SQL](https://www.oreilly.com/library/view/head-first-sql/9780596526849/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('../../data/film_testing.db')\n",
    "\n",
    "sql = \"\"\"\n",
    "    SELECT FilmID, FilmType, PropName, Direction, Measurement\n",
    "    FROM Measurements m\n",
    "    INNER JOIN Properties p ON m.PropertyID=p.PropertyID\n",
    "    \"\"\"\n",
    "\n",
    "df_sql = pd.read_sql(sql, conn)\n",
    "df_sql.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Selecting data from a `DataFrame`\n",
    "\n",
    "Selecting data from the `pd.DataFrame` is similar to what we have demonstrated for the Python `list` and `np.ndarray`, with a few key differences. First, we can select individual columns by using the column name. The row index value is maintained, and is still displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the entire column named 'fid'\n",
    "df['fid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also assign values to a column that you select by name. If the column doesn't already exist, it will be created. Here, we add a column to convert the time value from minutes to seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seconds'] = df['minutes'] * 60\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, we apply functions such as `DataFrame.head(n)` or `DataFrame.tail(n)` to just the selected column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fid'].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select specific rows, we need to access the `DataFrame.iloc` property, as shown below. Here, we use `iloc[10:20]` is used to get the slice of rows on the interval [10, 20). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More useful can be filtering based on values in the data itself. For instance, we can use a filter to select all rows of the `DataFrame` where the test time is greater than or equal to 10 minutes. Let's dissect this a little further. The inner expression, `df['minutes'] >= 10`, creates a series of `True`/`False` values, one for each row of the data set. This series of `True`/`False` is then used to select the rows of `df` in the outer expression. Any rows for which that condition is `True` are selected and returned. We can use any of the boolean operators introduced in Unit 1 to compare the values in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['minutes'] >= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Calculating basic statistics\n",
    "\n",
    "Just like the `np.ndarray`, the `pd.DataFrame` has basic statistical functions. If you execute these on the `DataFrame`, you'll apply the function to each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate statistics for every column in the DataFrame\n",
    "print(f'Minimum Value: {df.min()}')\n",
    "print(f'Maximum Value: {df.max()}')\n",
    "print(f'Mean: {df.mean()}')\n",
    "print(f'Standard Deviation: {df.std()}')\n",
    "print(f'Sum: {df.sum()}')\n",
    "print(f'Cumulative Sum: {df.cumsum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, as shown in the second example, we can select a column (as we learned above) and then apply the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate statistics for just the 'fid' column in the DataFrame\n",
    "print(f'Minimum Value: {df[\"fid\"].min()}')\n",
    "print(f'Maximum Value: {df[\"fid\"].max()}')\n",
    "print(f'Mean: {df[\"fid\"].mean()}')\n",
    "print(f'Standard Deviation: {df[\"fid\"].std()}')\n",
    "print(f'Sum: {df[\"fid\"].sum()}')\n",
    "print(f'Cumulative Sum: {df[\"fid\"].cumsum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method works, but `pandas` gives us an easier way. Using the `DataFrame.describe()` function you can more easily calculate the mean, min, max, stdev and quartiles of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value for the `DataFrame.describe()` function is, itself, a `DataFrame`. The names of the statistics (`count`, `mean`, ...) form the row index of this `DataFrame`. To select these values, use the `.loc` property of the DataFrame and provide the name of the statistic as an index value as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().loc['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Saving a `DataFrame` to a file\n",
    "\n",
    "It might be useful to save this result to a file to share with a colleague. Like `numpy`, we can easily save a `DataFrame` as a csv file. The two examples below are equivalent. In the first, we store the return value from the `DataFrame.describe()` function as a variable named `describe_df`. Then, in a second step, we save this to a file using the `DataFrame.to_csv()` function. This is useful if we need a copy of the result in memory, so that we can use it later on in our notebook. However, if we do not need to use the result again, we can use the second option which does not store the result in the computer's memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, store the result in a variable and then save\n",
    "describe_df = df.describe()\n",
    "describe_df.to_csv('../../output/unit2_dataframe.csv')\n",
    "\n",
    "# save result directly, without storing in a variable\n",
    "df.describe().to_csv('../../output/unit2_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Next Steps:\n",
    "\n",
    "1. Complete the [Unit 2 Problems](./unit02-problems.ipynb) to test your understanding\n",
    "2. Advance to [Unit 3](../03-basic-plotting/unit03-lesson.ipynb) when you're ready for the next step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2af3520cf2d099315c509e23b0f679c1508a4671732b5e09eee0e9586de4a344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
