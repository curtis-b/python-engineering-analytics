{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment: Unit 6\n",
    "--------------------\n",
    "\n",
    "Complete the problems below in your copy of the Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6.1.\n",
    "\n",
    "When conducting laboratory measurements, it is common practice to build a calibration curve by measuring the output of the lab instrument with standard samples. In the case of gas chromatography, this is required to determine the *response factor* for a specific chemical in the sensor. The file `gc_calibration.csv` contains calibration data for the measurement of hydrocarbons in polymer samples. The `Concentration` column represents the actual concentration of the hydrocarbon in ppm (mg hydrocarbon / kg polymer). The `FID_Signal` represents the corresponding area measured from the flame ionization detector (FID) signal.\n",
    "\n",
    "Perform a regression analysis on this model to predict the concentration from the FID signal, so that the concentration in future samples can be estimated.\n",
    "\n",
    "1. Test first, second and third order polynomial fits\n",
    "2. Measure the $R^2$ performance of each model\n",
    "3. Plot the raw data\n",
    "4. Overlay plots for each polynomial fit on the interval [0, 7000]\n",
    "\n",
    "Which model would you pick, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 6.1. solution\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# review the data to look at its structure\n",
    "cal_df = pd.read_csv('../../data/gc_calibration.csv')\n",
    "cal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data columns as individual numpy.ndarrays, reshaping x as a column vector\n",
    "cal_x = cal_df['FID_Signal'].to_numpy().reshape(-1, 1)\n",
    "cal_y = cal_df['Concentration'].to_numpy()\n",
    "\n",
    "# generate the 3 polynomial feature matrices (1=first order, ...)\n",
    "cal_X_1 = PolynomialFeatures(1).fit_transform(cal_x)\n",
    "cal_X_2 = PolynomialFeatures(2).fit_transform(cal_x)\n",
    "cal_X_3 = PolynomialFeatures(3).fit_transform(cal_x)\n",
    "\n",
    "# create and fit 3 regression models to the data\n",
    "# the intercept is included in the PolynomialFeatures\n",
    "cal_reg_1 = LinearRegression(fit_intercept=False)\n",
    "cal_reg_1.fit(cal_X_1, cal_y)\n",
    "\n",
    "cal_reg_2 = LinearRegression(fit_intercept=False)\n",
    "cal_reg_2.fit(cal_X_2, cal_y)\n",
    "\n",
    "cal_reg_3 = LinearRegression(fit_intercept=False)\n",
    "cal_reg_3.fit(cal_X_3, cal_y)\n",
    "\n",
    "# create an array to plot the predicted values [0, 7000]\n",
    "predict_fid = np.linspace(0, 7000, 50).reshape(-1, 1)\n",
    "\n",
    "# also need to transform this array of prediction values to a polynomial\n",
    "predict_fid_1 = PolynomialFeatures(1).fit_transform(predict_fid)\n",
    "predict_fid_2 = PolynomialFeatures(2).fit_transform(predict_fid)\n",
    "predict_fid_3 = PolynomialFeatures(3).fit_transform(predict_fid)\n",
    "\n",
    "# plot the raw data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(cal_x, cal_y)\n",
    "\n",
    "# don't forget to label the axes\n",
    "ax.set_xlabel('FID Peak Area')\n",
    "ax.set_ylabel('Concentration (mg/kg)')\n",
    "\n",
    "# plot each model prediction as a line\n",
    "ax.plot(predict_fid, cal_reg_1.predict(predict_fid_1), \n",
    "        label=f'1st Order ($R^2$={cal_reg_1.score(cal_X_1, cal_y):0.3f})')\n",
    "\n",
    "ax.plot(predict_fid, cal_reg_2.predict(predict_fid_2), \n",
    "        label=f'2nd Order ($R^2$={cal_reg_2.score(cal_X_2, cal_y):0.3f})')\n",
    "\n",
    "ax.plot(predict_fid, cal_reg_3.predict(predict_fid_3), \n",
    "        label=f'3rd Order ($R^2$={cal_reg_3.score(cal_X_3, cal_y):0.3f})')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three models show similar $R^2$ performance over this range of peak areas. All things being equal, I would select the simplest model that performs well, which would be the first order polynomial. It is always risky to use a model outside of the range of the training data. If values higher than 10 mg/kg are relevant to the experiment, I would recommend adding additional calibration points to cover this part of the range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6.2.\n",
    "\n",
    "Molten polymers typically behave as *pseudoplastic* or *shear-thinning*, non-Newtonian, fluids. This means that their viscosity is not constant, but depends on the shear rate that they are subjected to. For certain ranges of shear rates, the viscosity of a polymer melt may be reasonably described by the Power Law model, which states that: \n",
    "\n",
    "$$\\eta=K\\dot{\\gamma}^{n-1}$$\n",
    "\n",
    "For a shear-thinning fluid, $n<1$. \n",
    "\n",
    "The file `viscosity.csv` contains columns for shear-rate, $\\dot{\\gamma}$, in $rad/s$ and viscosity, $\\eta$, in $Pa\\cdot s$. Prepare this data and use linear regression to find the constants $K$ and $n$ which best fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem 6.2. solution setup\n",
    "\n",
    "First, we must transform the data so that a linear regression can be used. By taking the logarithm of both sides of the equation, we can linearize the equation:\n",
    "\n",
    "$$\\log \\eta = \\log \\left[ K \\dot{\\gamma}^{n-1} \\right]$$\n",
    "$$\\log \\eta = \\log K + \\left( n-1 \\right) \\log \\dot{\\gamma}$$\n",
    "\n",
    "By performing a regression on $\\log \\eta = f(\\log \\dot{\\gamma})$ we can calculate a slope, $n-1$, and intercept, $\\log K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 6.2. solution\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# load the data and look at the structure\n",
    "visc_df = pd.read_csv('../../data/viscosity.csv')\n",
    "visc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the log of each column\n",
    "visc_df['log-eta'] = np.log(visc_df['eta'])\n",
    "visc_df['log-gamma-dot'] = np.log(visc_df['gamma-dot'])\n",
    "\n",
    "# convert the data to numpy.ndarrays\n",
    "# optional as a separate step, but cleans up the code\n",
    "visc_x = visc_df['log-gamma-dot'].to_numpy().reshape(-1, 1)\n",
    "visc_y = visc_df['log-eta'].to_numpy()\n",
    "\n",
    "# use linear regression, fit intercept to simplify code\n",
    "visc_reg = LinearRegression(fit_intercept=True)\n",
    "visc_reg.fit(visc_x, visc_y)\n",
    "\n",
    "# calculate the constants from the slope and intercept\n",
    "K = np.exp(visc_reg.intercept_)\n",
    "n = visc_reg.coef_[0] + 1\n",
    "\n",
    "# plot the original data on a log-log scale\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(visc_df['gamma-dot'], visc_df['eta'])\n",
    "\n",
    "ax.loglog()\n",
    "ax.set_xlabel(r'$\\dot{\\gamma} (rad/s)$')\n",
    "ax.set_ylabel(r'$\\eta$ ($Pa\\cdot s$)')\n",
    "\n",
    "# plot the predicted values, remembering to convert by taking the exponential\n",
    "ax.plot(visc_df['gamma-dot'], np.exp(visc_reg.predict(visc_x)))\n",
    "\n",
    "# add text for the constants\n",
    "ax.text(100, 1000, f'K={K:0.3f}, n={n:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6.3.\n",
    "\n",
    "The file `film_classification_extended.csv` contains a greater number of features and observations for the BOPET v. BOPP classification problem that was discussed previously. Load this data set, generate a scaled feature matrix and array of class labels. Then, experiment with different neural network architectures to test the effect of nodes per layer and number of hidden layers. Describe your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 6.3. solution\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the data and look at the structure\n",
    "film_df = pd.read_csv('../../data/film_classification_extended.csv')\n",
    "film_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the X matrix from the data\n",
    "# we slice the array of column names to exclude the filmtype column\n",
    "film_X = film_df[film_df.columns[1:]].to_numpy()\n",
    "\n",
    "# scale the X matrix\n",
    "film_X_scaled = StandardScaler().fit_transform(film_X)\n",
    "\n",
    "# create an array of labels, 1 for BOPET and -1 for BOPP, using list comprehension\n",
    "film_labels = [1 if x=='BOPET' else -1 for x in film_df['filmtype']]\n",
    "\n",
    "# optional: create a loop to test many values\n",
    "# we will test multiples of 10 nodes per hidden layer\n",
    "nodes_per_layer = np.arange(10, 160, 10)\n",
    "\n",
    "# empty array to hold scores of 1, 2, 3, 4 layers x 15 nodes/layer values\n",
    "scores = np.zeros((4, len(nodes_per_layer)))\n",
    "\n",
    "# loop through the nodes_per_layer array to calculate score\n",
    "for i, n in enumerate(nodes_per_layer):\n",
    "\n",
    "    # create a 1-layer neural net\n",
    "    clf_nn_1 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                             hidden_layer_sizes=(n),\n",
    "                             random_state=1)\n",
    "\n",
    "    # test performance of the 1-layer model using cross-validation\n",
    "    scores[0, i] = cross_validate(clf_nn_1, film_X_scaled, film_labels)['test_score'].mean()\n",
    "\n",
    "    # create a 2-layer neural net\n",
    "    clf_nn_2 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                             hidden_layer_sizes=(n, n),\n",
    "                             random_state=1)\n",
    "\n",
    "    # test performance of the 2-layer model using cross-validation\n",
    "    scores[1, i] = cross_validate(clf_nn_2, film_X_scaled, film_labels)['test_score'].mean()\n",
    "\n",
    "    # create a 3-layer neural net\n",
    "    clf_nn_3 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                             hidden_layer_sizes=(n, n, n),\n",
    "                             random_state=1)\n",
    "\n",
    "    # test performance of the 3-layer model using cross-validation\n",
    "    scores[2, i] = cross_validate(clf_nn_3, film_X_scaled, film_labels)['test_score'].mean()\n",
    "\n",
    "    # create a 4-layer neural net\n",
    "    clf_nn_4 = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                             hidden_layer_sizes=(n, n, n, n),\n",
    "                             random_state=1)\n",
    "\n",
    "    # test performance of the 4-layer model using cross-validation\n",
    "    scores[3, i] = cross_validate(clf_nn_4, film_X_scaled, film_labels)['test_score'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the results of the test\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(nodes_per_layer, scores[0, :], label='1-layer')\n",
    "ax.plot(nodes_per_layer, scores[1, :], label='2-layer')\n",
    "ax.plot(nodes_per_layer, scores[2, :], label='3-layer')\n",
    "ax.plot(nodes_per_layer, scores[3, :], label='4-layer')\n",
    "\n",
    "ax.set_xlabel('Nodes per layer')\n",
    "ax.set_ylabel('Accuracy')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this test, we can see that increasing model complexity, especially with a small dataset, does not necessarily improve accuracy of the classifier. With 40+ nodes/layer, we are seeing accuracy of about 94% with 1-4 hidden layers. Further work could be done to pre-process (or possibly add new) features to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Next Steps:\n",
    "\n",
    "1. Advance to [Unit 7](../07-advanced-plotting/unit07-lesson.ipynb) when you're ready for the next step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2af3520cf2d099315c509e23b0f679c1508a4671732b5e09eee0e9586de4a344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
