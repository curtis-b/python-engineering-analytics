{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 6: Regression and Classification\n",
    "---------------------------------------\n",
    "\n",
    "**After completing this unit, you should be able to:**\n",
    "\n",
    "- Understand the theory that underpins least-squares regression\n",
    "- Compute the best fit polynomial for a dataset, using linear algebra\n",
    "- Perform a regression analysis using the *scikit-learn* package\n",
    "- Understand the purpose of a machine learning classification problem\n",
    "- Perform a basic two-class classification using the *scikit-learn* package\n",
    "- Understand and execute a *k-fold* cross-validation to test a classification algorithm\n",
    "\n",
    "## 6.1. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "n = 30\n",
    "\n",
    "# x values\n",
    "x = np.arange(0, n, 1)\n",
    "\n",
    "# y=2x+5 + random noise\n",
    "y = 2*x + 5 + 10*(np.random.random(n)-0.5)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2. Linear least-squares regression\n",
    "\n",
    "A simple and common approach to regression analysis is to find a linear function that approximates the data based on the available input variables. The equation of a line is $\\hat{y}=mx+b$, where we use $\\hat{y}$ to indicate that it is the predicted value of $y$. We can find the values of $m$ and $b$ that minimizes the error. We'll call the coefficients the *weights* and represent these as a vector, $w$. Using this notation, the equation of the line becomes:\n",
    "\n",
    "$$\\hat{y}=w_0x^0+w_1x^1$$\n",
    "\n",
    "where $w_0$ is the y-intercept and $w_1$ is the slope. For our regression, we have many experimental observations, which make up the rows in the matrix, $X$, below:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "| \\\\\n",
    "\\hat{y} \\\\\n",
    "|\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "| & | \\\\\n",
    "1 & x \\\\\n",
    "| & | \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_0 \\\\\n",
    "w_1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "When we multiply each row of $X$ by the weight vector, $w$, we get an estimate of $\\hat{y}$. Now, we need to pick the best $w$ to minimize the error. For least-squares regression, we define a *cost function* based on the Euclidian norm (2-norm) squared. The Euclidian norm of a column vector $v$ is $\\left( v^T v \\right)^{0.5}$. Note that this is the geometric distance. \n",
    "\n",
    "$$\\epsilon=\\Vert y-\\hat{y} \\Vert_2^2= \\Vert y-Xw \\Vert_2^2$$\n",
    "\n",
    "We want to find the optimum values for $w$ which minimize this error function. \n",
    "\n",
    "$$\\min_w \\Vert y-Xw \\Vert_2^2$$\n",
    "\n",
    "From calculus we know that, the global minimum of a convex function will occur where the derivative is equal to 0. \n",
    "\n",
    "$$\\min_{w} \\Vert y-Xw \\Vert_2^2 = 0$$\n",
    "$$\\nabla_w \\Vert y-Xw \\Vert_2^2 = 0$$\n",
    "\n",
    "$$\\nabla_w \\left[\\left( y-Xw \\right)^T \\left(y-Xw \\right) \\right] = 0$$\n",
    "$$\\nabla_w \\left[y^Ty - y^TXw - w^TX^Ty + w^TX^TXw \\right] = 0$$\n",
    "$$-X^Ty -X^Ty + 2X^TXw = 0$$\n",
    "$$X^TXw = X^Ty$$\n",
    "$$w=(X^TX)^{-1}X^Ty$$\n",
    "\n",
    "There you have it! The weights for a least-squares regression are simply the result of this matrix equation. Now, we will apply this formula to calculate the values for $w$ that minimize the squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix with ones in the first column, and our x values in the second column\n",
    "X = np.vstack((np.ones(n), x)).T\n",
    "\n",
    "# reshape the y values into a column vector\n",
    "y_col = y.reshape((n, 1))\n",
    "\n",
    "# compute the weights using the equation derived above\n",
    "# the np.linalg.inv function computes the matrix inverse\n",
    "w = np.linalg.inv(X.T@X)@X.T@y_col\n",
    "y_hat = X@w\n",
    "\n",
    "# plot the actual data and regression line\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, label='Actual Data')\n",
    "ax.plot(x, y_hat, label='$\\hat{y}$')\n",
    "\n",
    "# add the equation and legend\n",
    "ax.text(0, 45, f'$\\hat{{y}}$={w[1, 0]:0.2f}x+{w[0, 0]:0.2f}')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the resulting values for the slope and intercept are close to what we used to generate the data. To visualize what we have done, let's plot out the error as a function of $w$. We see in the following plot that the error forms a parabolic bowl. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of values that we will use to map out the slope, intercept\n",
    "n_weightvals = 500\n",
    "\n",
    "# array of possible slope values\n",
    "w1_arr = np.linspace(0, 4, n_weightvals)\n",
    "\n",
    "# array of possible y-intercept values\n",
    "w0_arr = np.linspace(-25, 35, n_weightvals)\n",
    "\n",
    "# meshgrid allows for 3d plotting\n",
    "# this forms two 2d matrices\n",
    "w1_mat, w0_mat = np.meshgrid(w1_arr, w0_arr)\n",
    "error = np.zeros(w1_mat.shape)\n",
    "\n",
    "for i in range(n_weightvals):\n",
    "    for j in range(n_weightvals):\n",
    "        e = (y - (w1_mat[i, j]*x + w0_mat[i, j])).reshape((n, 1))\n",
    "        error[i, j] = e.T@e\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot contour lines for the error function, clipped to a max of 5000\n",
    "ax.contour(w1_mat, w0_mat, np.clip(error, 0, 5000))\n",
    "\n",
    "ax.set_xlabel('Slope ($w_1$)')\n",
    "ax.set_ylabel('Intercept ($w_0$)')\n",
    "\n",
    "# plot the point calculated by our regression formula\n",
    "ax.scatter(w[1], w[0], c='black')\n",
    "ax.annotate(r'Best fit $\\nabla_w \\Vert\\epsilon\\Vert_2^2=0$', xy=w[::-1], xytext=[2.5, 20],  \n",
    "            arrowprops={'facecolor': 'black', 'width': 2})\n",
    "\n",
    "ax.text(0.1, -19, 'Contours show increasing error in $\\hat{y}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3. Leveraging the [`scikit-learn`](https://scikit-learn.org/stable/index.html) package\n",
    "\n",
    "As we have seen in previous units, there are often Python packages available to make our life easier. The `scikit-learn` package includes many useful models for regression and classification. For a simple linear model like this, the time savings is minor. But, as you get to more complicated models, it is much easier to use the pre-built models versus building your own.\n",
    "\n",
    "We will import individual modules from the `sklearn` library. For this example, we'll use the `LinearRegression` module. We first need to create a `LinearRegression` object. By default, the model will try to calculate an intercept term. However, we've already included that term in our $X$ matrix, so we set `fit_intercept=False`.\n",
    "\n",
    "There is a function of the regression model called `LinearRegression.fit()` that will calculate the weight vector, just as we have previously. Once the model has been fitted, we can use the `LinearRegression.predict()` function to apply the weights and generate a vector of $\\hat{y}$ values.\n",
    "\n",
    "If you want to print out, or make any additional calculations using the weight vector, this is available as the property `LinearRegression.coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create the LinearRegression object from scikit-learn\n",
    "# we set fit_intercept=False because we already have an intercept term in the X matrix\n",
    "reg = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# execute the regression operation\n",
    "reg.fit(X, y)\n",
    "y_hat = reg.predict(X)\n",
    "\n",
    "# plot the actual data and regression line\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y, label='Actual Data')\n",
    "ax.plot(x, y_hat, label='$\\hat{y}$')\n",
    "\n",
    "# add the equation and legend\n",
    "# these are stored in an array labeled .coef_ in the regression object\n",
    "ax.text(0.5, 45, f'$\\hat{{y}}$={reg.coef_[1]:0.2f}x+{reg.coef_[0]:0.2f}')\n",
    "ax.text(0.5, 41, f'$R^2$={reg.score(X, y):0.2f}')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4. Multi-variate linear regression\n",
    "\n",
    "More generally, we can develop a linear regression with any number, $n$, of input variables. There is actually no change to the formula that we derived above. \n",
    "\n",
    "$$\\hat{y_i}=w_0x_{i0}+w_1x_{i1}+...+w_nx_{in}$$\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "| \\\\\n",
    "\\hat{y} \\\\\n",
    "|\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "| & | & | & | \\\\\n",
    "x_0 & x_1 & ... & x_n \\\\\n",
    "| & | & | & |\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "| \\\\\n",
    "w \\\\\n",
    "|\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Even though this is a *linear* regression, we can include polynomial terms. To do this, create additional columns in the $X$ matrix. \n",
    "\n",
    "We could generate this matrix directly using `numpy`, but `scikit-learn` has a module available to generate polynomial expansions of a feature. In the example below, we have a dataset that appears to follow a parabolic shape, so we add a squared term to the regression. Create a `PolynomialFeatures` object. The parameter that you pass should be the maximum order of the polynomial (in this case, 2). Then, run the `PolynomialFeatures.fit_transform(x)` function to generate a matrix that has (in this case) $\\begin{bmatrix} x^0 &  x^1 & x^2 \\end{bmatrix}$ as the columns.\n",
    "\n",
    "The first 10 rows are printed out so that you can see the matrix that was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x = np.arange(0, n, 1)\n",
    "y = x**2 + 100 + 100*(np.random.random(n)-0.5)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "X = poly.fit_transform(x.reshape((n, 1)))\n",
    "X[:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One we have the feature matrix, $X$, the linear regression is identical to what was done previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn LinearRegression object\n",
    "reg = LinearRegression(fit_intercept=False)\n",
    "reg.fit(X, y)\n",
    "\n",
    "# plot the raw data and regression fit\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x, reg.predict(X))\n",
    "\n",
    "# display the equation on the axis\n",
    "equation_text = ' '.join([f'{a:+0.2f}$x^{{{i}}}$' for i, a in enumerate(reg.coef_)])\n",
    "ax.text(0.5, 820, f'y={equation_text}')\n",
    "ax.text(0.5, 720, f'$R^2$={reg.score(X, y):0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go too far with adding polynomial terms. As the number of terms increases, the $R^2$ will continue to improve. However, we risk *overfitting* the data. This means that we may be fitting noise in the data, which will increase error when we compare to data that is not part of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create polynomial features up to a power of 7\n",
    "poly = PolynomialFeatures(7)\n",
    "X = poly.fit_transform(x.reshape((n, 1)))\n",
    "\n",
    "# complete the linear regression\n",
    "reg = LinearRegression(fit_intercept=False)\n",
    "reg.fit(X, y)\n",
    "\n",
    "# plot the raw data and regression fit\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x, reg.predict(X))\n",
    "\n",
    "# display the equation on the axis\n",
    "equation_text = ''.join([f'{a:+0.1f}$x^{{{i}}}$' for i, a in enumerate(reg.coef_)])\n",
    "ax.text(0.5, 820, f'y={equation_text}')\n",
    "ax.text(0.5, 720, f'$R^2$={reg.score(X, y):0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5. Other regression techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Classification\n",
    "\n",
    "The techniques we use for regression analysis can be applied to the problem of classification. Consider the example below, where we have points identified by the colors gray and green. These represent two classes of objects. A machine learning classification teaches the computer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "x0 = np.random.random(N)*80\n",
    "x1 = np.random.random(N)*60\n",
    "\n",
    "category = np.array([1 if x1 > 0.5*x0+10 else -1 for x0, x1 in zip(x0, x1)])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x0, x1, c=category, cmap='Dark2')\n",
    "\n",
    "ax.set_xlabel(r'$x_0$')\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1. Linear classification\n",
    "\n",
    "$$\\text{sign}\\left( X^Tw \\right)=$$\n",
    "\n",
    "The decision boundary is where $X^Tw=0$. This is the equation $w_0x_0+w_1x_1+w_2=0$, so we can rearrange the equation to plot the decision boundary in terms of $\\left(x_0,x_1\\right)$. \n",
    "\n",
    "$$x_1=-\\frac{w_0}{w_1}x_0-\\frac{w_2}{w_1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((x0, x1, np.ones(N))).T\n",
    "y = category.reshape((N, 1))\n",
    "\n",
    "w = np.linalg.inv(X.T@X)@X.T@y\n",
    "m = -w[0]/w[1]\n",
    "b = -w[2]/w[1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(x0, x1, c=category, cmap='Dark2')\n",
    "ax.plot(x0, m*x0+b, ls='--')\n",
    "\n",
    "ax.set_xlabel(r'$x_0$')\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2. Support vector machines\n",
    "\n",
    "Sensitivity linear classifier to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "clf_svm = svm.SVC()\n",
    "\n",
    "clf_svm.fit(np.vstack((x0, x1)).T, category)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    clf_svm, np.vstack((x0, x1)).T, response_method='predict',\n",
    "    alpha=0.2, ax=ax\n",
    ")\n",
    "\n",
    "ax.scatter(x0, x1, c=category, cmap='Dark2')\n",
    "\n",
    "ax.set_xlabel(r'$x_0$')\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3. Deep learning (neural networks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_nn = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 5, 5), \n",
    "                    random_state=1)\n",
    "\n",
    "clf_nn.fit(np.vstack((x0, x1)).T, category)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(\n",
    "    clf_nn, np.vstack((x0, x1)).T, response_method='predict',\n",
    "    alpha=0.2, ax=ax\n",
    ")\n",
    "\n",
    "ax.scatter(x0, x1, c=category, cmap='Dark2')\n",
    "\n",
    "ax.set_xlabel(r'$x_0$')\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Testing the performance of machine learning algorithms\n",
    "\n",
    "k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "    \n",
    "acc_svm = cross_validate(clf_svm, np.vstack((x0, x1)).T, category)['test_score'].mean()\n",
    "acc_nn = cross_validate(clf_nn, np.vstack((x0, x1)).T, category)['test_score'].mean()\n",
    "\n",
    "acc_svm, acc_nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Next Steps:\n",
    "\n",
    "1. Complete the [Unit 6 Problems](./unit06-solutions.ipynb) to test your understanding\n",
    "2. Advance to [Unit 7](../07-advanced-plotting/unit07-lesson.ipynb) when you're ready for the next step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2857618fbf4d4bb3d9a88a0331cc9f5539aea2b45b45163c4b048952b7884495"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
