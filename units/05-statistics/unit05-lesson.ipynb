{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 5: Statistical Analysis\n",
    "------------------------------\n",
    "\n",
    "- Calculate and plot the histogram for a dataset\n",
    "- Calculate random samples from a distribution\n",
    "- Calculate a confidence interval\n",
    "- Central limit theorem\n",
    "- Create an x-bar R run chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Calculating and plotting a histogram\n",
    "\n",
    "The histogram is a common visualization taught in statistics classes. It helps us to summarize a large set of values to observe the spread of a data set. To generate a histogram, we first define a set of *bins* that are relevent for our dataset. For example, if we were looking at ages of people, we might consider bins with a width of 10 years (0-9 years, 10-19 years, 20-29 years, ...). Then, as you go through the data, you will count the number of entries that fall within each bin. The results of this process are typically visualized as a bar graph, with no space in between the bars.\n",
    "\n",
    "### 5.1.1. Plotting the histogram using `matplotlib`\n",
    "\n",
    "In the example below, we load a set of tensile modulus measurements for polymer films. The `matplotlib` package provides a function `axis.hist(x)` that can be used to easily plot a histogram for a set of data. This function will figure out the bin size and spacing on its own. By viewing the sample below, we can quickly identify that this dataset appears to have two separate distributions, which may indicate that it represents two different polymer films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "modulus = np.loadtxt('../../data/bopet_modulus-MPa.csv')\n",
    "\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "ax.hist(modulus)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have options in how we can set up the bins. In addition to the data array. The `axis.hist()` function has an additional parameter called `bins` that can be used to either set a number of bins, or to fix the bin edges. In the example below, we set the bin count to 50. While it might seem like more bins will always improve resolution, there is a point at which you make the bins too narrow and you lose sight of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots() \n",
    "\n",
    "ax.hist(modulus, bins=50, density=True)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can set the actual bin edges. Recall the `np.arange()` function that we learned in Unit 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = np.arange(4300, 5700, 100)\n",
    "\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "ax.hist(modulus, bins=bin_edges)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the histogram values in `numpy`\n",
    "\n",
    "Sometimes, it is helpful to have the actual data, so that you can do something with it, like plotting a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins = np.histogram(modulus, bins=bin_edges)\n",
    "\n",
    "counts, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots() \n",
    "\n",
    "counts, bins, bars = ax.hist(modulus, bins=bin_edges)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "counts, bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2. Adding the cumulative sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots() \n",
    "\n",
    "counts, bins, bars = ax.hist(modulus, bins=bin_edges)\n",
    "ax.plot(bins[0:-1] + 50, counts.cumsum())\n",
    "\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots() \n",
    "\n",
    "counts, bins, bars = ax.hist(modulus, bins=bin_edges)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(bins[0:-1] + 50, counts.cumsum())\n",
    "\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots() \n",
    "\n",
    "counts, bins, bars = ax.hist(modulus, bins=bin_edges, alpha=0.5)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_ylim((0, 12))\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(bins[0:-1] + 50, counts.cumsum())\n",
    "ax2.set_ylabel('Cumulative Sum')\n",
    "ax2.set_ylim((0, 60))\n",
    "ax2.grid(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3. Overlaying the normal distribution\n",
    "\n",
    "In the Unit 1 problems, we created a custom function for the Gaussian (normal) continuous probability distribution.\n",
    "\n",
    "$$f(x)=\\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}\\frac{\\left(x-\\mu\\right)^2}{\\sigma^2}}$$\n",
    "\n",
    "While this method works, it's more efficient to assume that someone else has already created a module to address common needs like this. The `scipy` project includes a package for statistical analysis called [`scipy.stats`](https://docs.scipy.org/doc/scipy/tutorial/stats.html), which includes many common probability distributions.\n",
    "\n",
    "In this example, we import the package using the line `from scipy import stats`, which imports only the `stats` package from the broader `scipy` project. In the subsequent code, we will preference any calls from this package with `stats.`.\n",
    "\n",
    "Here, we create a normal probability distribution object, with a given mean and standard deviation. In the `scipy.stats` package, the paramater `loc` is the mean and `scale` is the standard deviation. Then, we use the `norm.pdf(x)` function to calculate the value of the probability density function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "counts, bins, bars = ax.hist(modulus, bins=bin_edges, density=True, alpha=0.5)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "x = np.linspace(modulus.min()-modulus.std(), modulus.max()+modulus.std(), 100)\n",
    "\n",
    "# create an normal probability distribution object\n",
    "norm_dist = stats.norm(loc=modulus.mean(), scale=modulus.std())\n",
    "\n",
    "# calculate the value of the probability density function for each value in x\n",
    "norm_pdf = norm_dist.pdf(x)\n",
    "\n",
    "# overlay the probability density function\n",
    "ax.plot(x, norm_pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots() \n",
    "\n",
    "counts, bins, bars = ax.hist(modulus, bins=bin_edges, density=True, alpha=0.5)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "x = np.linspace(modulus.min()-modulus.std(), modulus.max()+modulus.std(), 100)\n",
    "pdf = stats.norm.pdf(x, loc=modulus.mean(), scale=modulus.std())\n",
    "\n",
    "\n",
    "lower_limit = modulus.mean() - modulus.std()\n",
    "upper_limit = modulus.mean() + modulus.std()\n",
    "\n",
    "ax.plot(x, pdf)\n",
    "ax.fill_between(x, 0, pdf, where=((x>lower_limit) & (x<upper_limit)), alpha=0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Constructing a run chart of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(np.arange(len(modulus)), modulus, marker='x')\n",
    "ax.set_xlabel('Index of Data Point')\n",
    "ax.set_ylabel('Modulus (MPa)')\n",
    "\n",
    "mean_first30 = modulus[:30].mean()\n",
    "std_first30 = modulus[:30].std()\n",
    "\n",
    "ax.axhline(mean_first30+2*std_first30, ls='--')\n",
    "ax.axhline(mean_first30-2*std_first30, ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Testing 2 separate distributions\n",
    "\n",
    "Based on what we learned in the previous section, we may want to conduct a statistical test to determine if these two sets of data are likely from the same distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "# plotting the first and second half of the data separately\n",
    "# here, setting the transparency (alpha) to 40% helps visualize overlap\n",
    "ax.hist(modulus[:30], bins=bin_edges, density=True, color='red', alpha=0.4)\n",
    "ax.hist(modulus[30:], bins=bin_edges, density=True, color='blue', alpha=0.4)\n",
    "\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "# overlay the standard normal distributions for the two sets\n",
    "x = np.linspace(modulus.min()-modulus.std(), modulus.max()+modulus.std(), 100)\n",
    "\n",
    "pdf_first30 = stats.norm.pdf(x, loc=modulus[:30].mean(), scale=modulus[:30].std())\n",
    "pdf_last30 = stats.norm.pdf(x, loc=modulus[30:].mean(), scale=modulus[30:].std())\n",
    "\n",
    "ax.plot(x, pdf_first30, color='red')\n",
    "ax.plot(x, pdf_last30, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Additional features of a probability distribution\n",
    "\n",
    "### 5.4.1. Cumulative probability distributions\n",
    "\n",
    "In addition to the probability density function, `norm.pdf(x)`, `scipy.stats` provides other useful functions for working with these probability distributions. For instance, for any continuous probability distribution function, we can define the continuous distribution function as\n",
    "\n",
    "$$cdf(x)=\\int\\limits_{-\\infty}^{x}pdf(x)dx$$\n",
    "\n",
    "As you can see, this is the probability of any value less than `x` ($P<x$). This can be calculated by using the function `norm.cdf(x)`. \n",
    "\n",
    "Alternatively, we may also need to calculate the probability of a value greater than `x` ($P>x$), which uses the *survival function*, `norm.sf(x)`. Because the total area of the pdf function is equal to 1, we can look at these as equal.\n",
    "\n",
    "$$sf(x)=\\int\\limits_{x}^{\\infty}pdf(x)dx=1-\\int\\limits_{-\\infty}^{x}pdf(x)dx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dist = stats.norm(loc=10, scale=0.6)\n",
    "\n",
    "x = np.linspace(7, 13, 100)\n",
    "norm_pdf = norm_dist.pdf(x)\n",
    "norm_cdf = norm_dist.cdf(x)\n",
    "norm_sf = norm_dist.sf(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, norm_pdf, label='pdf')\n",
    "ax.plot(x, norm_cdf, label='cdf')\n",
    "ax.plot(x, norm_sf, label='sf')\n",
    "\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the inverse of the `cdf` and `isf` functions available.\n",
    "\n",
    "| Function | Inverse | Description |\n",
    "|----------|---------|-------------|\n",
    "| `cdf(x)` | `ppf(p)` | *percentage point function*: at what *X'* is $P(x<X')=p$ |\n",
    "| `sf(x)` | `isf(p)` | *inverse survival function*: at what *X'* is $P(x>X')=p$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, norm_pdf, lw=2, label='pdf')\n",
    "ax.plot(x, norm_cdf, label='cdf')\n",
    "ax.plot(x, norm_sf, label='sf')\n",
    "\n",
    "x_ppf = norm_dist.ppf(0.9)\n",
    "ax.axvline(x_ppf, ls='--', label='ppf(0.9)')\n",
    "ax.text(x_ppf+0.1, 0.8, 'ppf(0.9)', rotation='vertical')\n",
    "\n",
    "x_isf = norm_dist.isf(0.8)\n",
    "ax.axvline(x_isf, ls='--', label='isf(0.8)')\n",
    "ax.text(x_isf+0.1, 0.8, 'isf(0.8)', rotation='vertical')\n",
    "\n",
    "\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2. Sampling random values from the distribution\n",
    "\n",
    "We may also want to run an experiment by generating random values from the normal distribution. Recall that in Unit 2, we created random values from a flat probability distribution using `numpy.random`. We can use the normal probability distribution object created in the cell above, and generate an array of random values from the distribution using the function `norm.rvs(size=n)` (random values). This function takes a parameter named `size` which is the length of the array that will be generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_random = norm_dist.rvs(size=200)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(norm_random, density=True)\n",
    "ax.plot(x, norm_pdf, label='pdf')\n",
    "\n",
    "ax.set_ylabel('Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. Measurement Variability\n",
    "\n",
    "### 5.5.1 Central Limit Theorem\n",
    "\n",
    "Consider a set of laboratory measurements. We already know how to calculate the mean and standard deviation of the data. What do we do if there is a high degree of variability in the data? Intuitively, we might take multiple measurements, and average them together, to reduce noise in the data. \n",
    "\n",
    "The [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) describes what happens if we randomly draw $n$ samples from population with a mean, $\\mu$, and standard deviation, $\\sigma$. We can average the samples to get the variable $\\bar{x}_n$. The standard deviation of the averages will decrease as $n$ increases:\n",
    "\n",
    "$$\\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "So, as we average more samples, we reduce variability and improve the power of our measurement. An illustration of the effect of averaging is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vals = 1200\n",
    "norm_random = stats.norm().rvs(size=total_vals)\n",
    "\n",
    "# plot the histogram, with the normal pdf\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim((-3.5, 3.5))\n",
    "\n",
    "x = np.linspace(-3.5, 3.5, 100)\n",
    "\n",
    "# number of measurements to average together\n",
    "n_to_average = [1, 5, 10, 50]\n",
    "\n",
    "for n in n_to_average:\n",
    "\n",
    "    # reshape the 1d array into a 2d matrix (number of cols = n)\n",
    "    norm_reshape = norm_random.reshape((int(total_vals/n), n))\n",
    "\n",
    "    # take the mean across the replicate measurements in each row\n",
    "    norm_means = norm_reshape.mean(axis=1)\n",
    "\n",
    "    # add a histogram of the averaged values to the axis\n",
    "    counts, bins, patches = ax.hist(norm_means, density=True, alpha=0.5, \n",
    "                                    label=f'$\\\\bar{{x}}_{{{n}}}$')\n",
    "    \n",
    "    # get the color to match the normal pdf to the histogram\n",
    "    color = patches[0].get_facecolor()\n",
    "\n",
    "    # add the standard normal pdf to the axis\n",
    "    ax.plot(x, stats.norm(loc=0, scale=1/np.sqrt(n)).pdf(x), c=color, alpha=1)\n",
    "\n",
    "ax.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2. Determining significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Next Steps:\n",
    "\n",
    "1. Complete the [Unit 5 Problems](./unit05-solutions.ipynb) to test your understanding\n",
    "2. Advance to [Unit 6](../06-regression-classification/unit06-lesson.ipynb) when you're ready for the next step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2af3520cf2d099315c509e23b0f679c1508a4671732b5e09eee0e9586de4a344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
