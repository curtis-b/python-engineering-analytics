{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 5: Statistical Analysis\n",
    "------------------------------\n",
    "\n",
    "Understanding statistical analysis helps the research engineer understand significance of an experimental result and the manufacturing engineer understand process variability. However, in my experience there has been less emphasis on statistical analysis in research when compared to manufacturing / quality.\n",
    "\n",
    "In this section, we will introduce several basic statistical concepts, using the `scipy.stats` Python package. This section will assume a basic understanding of statistical terms such as mean, median, mode, variance and standard deviation.\n",
    "\n",
    "**After completing this unit, you should be able to:**\n",
    "\n",
    "- Calculate and plot the histogram for a dataset\n",
    "- Generate random samples from a probability distribution\n",
    "- Understand the Central Limit Theorem, and implications on variance in experimental data\n",
    "- Create an x-bar R run chart\n",
    "- Conduct a t-test and calculate a confidence interval for a sample mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Calculating and plotting a histogram\n",
    "\n",
    "The histogram is a common visualization taught in introductory statistics classes to summarize large sets of measurements. Instead of looking at each individual data point, we define a set of *bins* that are relevent for our dataset. For example, if we were looking at ages of people, we might consider bins with a width of 10 years (0-9 years, 10-19 years, 20-29 years, ...). Then, the number of measurements falling within each bin is counted. The results of this process are typically visualized as a bar graph with no space in between the bars. We may optionally choose to divide each count by the total number of observations to scale the y-axis. Scaling by the total number of values can make it easier to compare the shape of two sets of data that may have different counts.\n",
    "\n",
    "### 5.1.1. Plotting the histogram using `matplotlib`\n",
    "\n",
    "In the example below, we load a set of tensile modulus measurements for polymer films into an `np.ndarray`. Try adding code to the cell below to determine how many values are in this data set. With the data loaded into memory, the `matplotlib` function [`axis.hist()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.hist.html) can easily plot the histogram of the supplied data. By default, the data will be split into 10 bins, evenly spaced between the minimum and maximum values of the data. Starting with these default settings we can quickly identify that this dataset appears to be bi-modal (comprising two separate distributions), which *may* indicate that it represents two polymer films (or lots of film) that are statistically different. We will further analyze this distribution later in the file to determine if there is evidence (beyond visual appearance) to support this assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# load the modulus dataset into memory as a 1d array\n",
    "modulus = np.loadtxt('../../data/bopet_modulus-MPa.csv')\n",
    "\n",
    "# create the plot\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "# add the histogram to the axis\n",
    "ax.hist(modulus)\n",
    "\n",
    "# update the axis labels\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2. Adjusting the histogram bins\n",
    "\n",
    "Using the `bins` parameter of the `axis.hist()` function, we can set either a number of bins (as an `integer`), the bin edges (as a `list` or `np.ndarray`). Study the histogram below, with the `bins` parameter set to 50. Modify the bin count and observe how the histogram changes. While it might seem like more bins will always improve resolution, there is a point at which you make the bins too narrow and you lose sight of the distribution. This examples also demonstrates the `density` parameter, which can be set to `True` to scale the histogram by the total number of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots() \n",
    "\n",
    "ax.hist(modulus, bins=50, density=True)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we set a bin count, the width of the bins will scale with the range of the data set. This isn't usually a problem with a single set of data, but if we are plotting multiple histograms, of comparing the histograms from different points in time, we may want to enforce greater consistency in how the bins are created. \n",
    "\n",
    "Alternatively, we can supply the actual (left) bin edges in place of the bin count. Recall the `np.arange()` function that we learned in Unit 2, and modify the code if needed to print out the bin edge array that has been created here. Here, we create bins with a width of 100 MPa, beginning at 4200 MPa and ending with 5800. Creating bins using a round number like this can make it easier to read data from the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of bin edges to supply to the histogram\n",
    "bin_edges = np.arange(4200, 5801, 100)\n",
    "\n",
    "# plot the histogram using the provided bin_edges array\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "ax.hist(modulus, bins=bin_edges)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bin edge array form a series of half-open intervals for the bins: $\\left[4300,4400\\right)$, $\\left[4400,4500\\right)$, ... However, the final bin will *include* the last value: $\\left[5700,5800\\right]$. If you are creating your own bins, it is important to know that any values outside of the bin range will be clipped from the data. The same modulus data is used to create the histogram below, but the `bin_edges` array does not capture the full range of the data. There is no warning that this occurs, so be careful when coding the bin ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of bin edges to supply to the histogram\n",
    "bin_edges = np.arange(4600, 5001, 100)\n",
    "\n",
    "# plot the histogram using the provided bin_edges array\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "ax.hist(modulus, bins=bin_edges)\n",
    "\n",
    "ax.set_xticks(bin_edges)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best of both worlds, you can write code to calculate the `bin_edges` array based on your actual data set. In the code below, the functions `np.floor()` (round down to the nearest integer) and `np.ceil()` (round up to the nearest integer) are used to determine the proper bin edges to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the bin width\n",
    "bin_width = 100\n",
    "\n",
    "# calculate the minimum bin edge, rounded down to the nearest bin width\n",
    "bin_min = bin_width * np.floor(modulus.min() / bin_width)\n",
    "\n",
    "# calculate the minimum bin edge, rounded up to the nearest bin width\n",
    "bin_max = bin_width * (np.ceil(modulus.max() / bin_width) + 1)\n",
    "\n",
    "# use the calculated values to create the bin_edges array\n",
    "bin_edges = np.arange(bin_min, bin_max, bin_width)\n",
    "bin_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also different automated approaches for the creation of bins, which are describe in the [`numpy` documentation]((https://numpy.org/doc/stable/reference/generated/numpy.histogram_bin_edges.html#numpy.histogram_bin_edges)). To use one of these approaches, provide the algorithm name as the value of the `bins` parameter as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram using the sturges method of setting bin count\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "ax.hist(modulus, bins='sturges')\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3. The numerical histogram values\n",
    "\n",
    "Sometimes, it is helpful to save the numerical results of the histogram operation. For example, we could apply the `ndarray.cumsum()` function to calculate and plot the cumulative sum of the bin counts. The `axis.hist()` function returns a `tuple` of three values:\n",
    "\n",
    "1. An array of bin counts. These will be normalized by the total count if `density=True`.\n",
    "2. The array of bin edges, generated by any of the methods described above.\n",
    "3. An object containing the bars generated on the axis by `matplotlib`.\n",
    "\n",
    "We will focus on the first return value -- the array of counts by bin. Modify the `bins` parameter and see how this changes the `counts` and `bins` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram using the pre-defined bin edges from previous cells\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "counts, bins, bars = ax.hist(modulus, bins=bin_edges)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# display the counts and bins arrays as a tuple\n",
    "counts, bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4. Adding the cumulative sum to the histogram\n",
    "\n",
    "Using the `counts` array, we can calculate and plot the cumulative distribution. In this example, we are adding the cumulative distribution as a line, and centering the points within the bin range. Spend some time looking at the code to create the x-coordinate. This averages between the left and right edge of each bin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram using the pre-defined bin edges from previous cells\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "counts, bins, bars = ax.hist(modulus, bins=bin_edges)\n",
    "\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# calculate the cumulative sum for the y-value\n",
    "cum_counts = counts.cumsum()\n",
    "\n",
    "# calculate the center point of each bin for the x-coordinate\n",
    "# this is calculated by stacking the 0...n-1 bin edges (left edges) with\n",
    "# the array of 1...n bin edges (right edges) and computing \n",
    "# the mean by column (axis=0)\n",
    "cum_x = np.vstack((bins[:-1], bins[1:])).mean(axis=0)\n",
    "\n",
    "# plot the cumulative sum as a line\n",
    "ax.plot(cum_x, cum_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the cumulative sum is obviously much larger than the count value in any one bin, the plot above isn't very effective. The histogram is now compressed to the bottom of the plot. One approach might be to use separate left and right y-scales, as you might do in a spreadsheet graph. The function `axis.twinx()` creates a duplicate of the axis, overlaid with the original axis and sharing x-coordinates. The y-axis scale can be different, and the y-ticks will be shown (by default) on the right side of the graph. The second axis is captured as a new variable name and used to plot the cumulative sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram using the pre-defined bin edges from previous cells\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "counts, bins, bars = ax.hist(modulus, bins=bin_edges)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# create a duplicate axis, overlaid on the first, sharing the same x\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "# plot the cumulative sum on axis 2 as a line\n",
    "ax2.plot(cum_x, cum_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an ugly plot because the default number of ticks is not the same between the two y scales. As a result, we have multiple sets of gridlines that are both confusing and aesthetically displeasing. When using twinned axes in `matplotlib`, it is usually necessary to customize the y limits and tick marks to align the two scales. In the example below, the scales are modified to have a 1:5 ratio between the left and right scales. This results in a plot that is relatively easy to read and understand, where the grid lines are useful for both the left and right scales. The `alpha` parameter is also used to fade the data into the background, which could be used to highlight the cumulative distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram using the pre-defined bin edges from previous cells\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "counts, bins, bars = ax.hist(modulus, bins=bin_edges, alpha=0.4)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "# fix the left y-axis scale\n",
    "ax.set_ylim((0, 13))\n",
    "ax.set_yticks(np.arange(0, 13, 2))\n",
    "\n",
    "# create a duplicate axis and plot the cumulative distribution\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(cum_x, cum_counts)\n",
    "ax2.set_ylabel('Cumulative Sum')\n",
    "\n",
    "# fix the right y-axis scale\n",
    "ax2.set_ylim((0, 65))\n",
    "ax2.set_yticks(np.arange(0, 61, 10))\n",
    "\n",
    "# turn off the gridlines on the second axis so that they don't overlay\n",
    "# the histogram plotted on ax\n",
    "ax2.grid(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5. Overlaying the normal probability distribution\n",
    "\n",
    "In the Unit 1 problems, we created a custom function for the Gaussian (normal) continuous probability distribution.\n",
    "\n",
    "$$f(x)=\\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}\\frac{\\left(x-\\mu\\right)^2}{\\sigma^2}}$$\n",
    "\n",
    "While this method works, it's more efficient to assume that someone else has already created a module to address common needs like this. The `scipy` project includes a package for statistical analysis called [`scipy.stats`](https://docs.scipy.org/doc/scipy/tutorial/stats.html), which includes many common probability distributions. In this example, we import the package using the line `from scipy import stats`, which imports only the `stats` package from the broader `scipy` project. In the subsequent code, we will preference any calls from this package with `stats.`.\n",
    "\n",
    "Here, we calculate values of the a normal probability distribution function for a given mean and standard deviation. In the `scipy.stats` package, the parameter `loc` (location) is the distribution mean and `scale` is the standard deviation. For the standard deviation, we set `ddof=1` to indicate that we are using the sample standard deviation. The `norm.pdf()` function calculates the value of the probability density function at each point in the provided `x` array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# plot the histogram using the pre-defined bin edges from previous cells\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "counts, bins, bars = ax.hist(modulus, bins=bin_edges, density=True, alpha=0.5)\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "# create a set of 100 x values at which to evalute the normal pdf\n",
    "# the space is created 1 stdev above and below the range of the data\n",
    "x = np.linspace(modulus.min()-modulus.std(ddof=1), modulus.max()+modulus.std(ddof=1), 100)\n",
    "\n",
    "# calculate the value of the probability density function for each value in x\n",
    "norm_pdf = stats.norm.pdf(x, loc=modulus.mean(), scale=modulus.std(ddof=1))\n",
    "\n",
    "# overlay the probability density function in the plot\n",
    "ax.plot(x, norm_pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Additional features of the normal probability distribution\n",
    "\n",
    "### 5.2.1. Cumulative probability distributions\n",
    "\n",
    "In addition to the probability density function, `norm.pdf(x)`, `scipy.stats` provides other useful functions for working with these probability distributions. For instance, for any continuous probability distribution function, we can define the continuous distribution function as\n",
    "\n",
    "$$\\text{cdf}(x)=\\int\\limits_{-\\infty}^{x}\\text{pdf}(x)dx$$\n",
    "\n",
    "As you can see, this is the probability of any value less than $x$ ($P<x$). As $x \\rightarrow \\infty$, the $\\text{cdf} \\rightarrow 1$; the area under the entire curve is 1. The value of the cdf can be calculated by using the function `norm.cdf(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of x values for plotting\n",
    "x = np.linspace(7, 13, 100)\n",
    "\n",
    "# evaluate the pdf, cdf functions at x\n",
    "norm_pdf = stats.norm.pdf(x, loc=10, scale=0.6)\n",
    "norm_cdf = stats.norm.cdf(x, loc=10, scale=0.6)\n",
    "\n",
    "# plot the pdf, cdf functions\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, norm_pdf, label='pdf')\n",
    "ax.plot(x, norm_cdf, label='cdf')\n",
    "\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we may also need to calculate the probability of a value greater than $x$ ($P>x$), which uses the *survival function*, `norm.sf(x)`. Because the total area of the pdf function is equal to 1, we know that:\n",
    "\n",
    "$$\\text{sf}(x)=\\int\\limits_{x}^{\\infty}\\text{pdf}(x)dx=1-\\int\\limits_{-\\infty}^{x}\\text{pdf}(x)dx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the pdf, sf functions at x (using the prior x values)\n",
    "norm_pdf = stats.norm.pdf(x, loc=10, scale=0.6)\n",
    "norm_sf = stats.norm.sf(x, loc=10, scale=0.6)\n",
    "\n",
    "# plot the pdf, sf functions\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, norm_pdf, label='pdf')\n",
    "ax.plot(x, norm_sf, label='sf')\n",
    "\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the inverse of the `cdf` and `isf` functions available in `scipy.stats`. These can be used to answer questions like, \"at what value of $x$ is the $\\text{cdf}(x)<0.2$?\" Previously, you may have answered this type of question with standard tables of the normal distribution, but we can calculate this answer directly.\n",
    "\n",
    "| Function | Inverse | Description |\n",
    "|----------|---------|-------------|\n",
    "| `cdf(x)` | `ppf(q)` | *percentage point function*: at what *X'* is $P(x<X')=p$ |\n",
    "| `sf(x)` | `isf(q)` | *inverse survival function*: at what *X'* is $P(x>X')=p$ |\n",
    "\n",
    "Because $\\text{cdf}=1-\\text{sf}$, $\\text{ppf}(q)=\\text{isf}(1-q)$.\n",
    "\n",
    "This example calculates the measurement value corresponding to $\\text{cdf}(x)<0.2$, and introduces the [`axis.fill_between()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.fill_between.html) function to shade the area under the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the pdf, cdf functions using previously calculated values\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, norm_pdf, label='pdf')\n",
    "\n",
    "# the zorder parameter is used to make sure that the cdf is displayed above the fill\n",
    "ax.plot(x, norm_cdf, label='cdf', zorder=10)\n",
    "\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "\n",
    "# calculate the x value where cdf(x)=0.2\n",
    "x_20 = stats.norm.ppf(0.2, loc=10, scale=0.6)\n",
    "\n",
    "# plot a vertical line at the x_20 value\n",
    "ax.axvline(x_20, ls='--')\n",
    "\n",
    "# fill the area of the pdf up to the x_20 value\n",
    "# use array filters to select the proper points to plot\n",
    "# use zorder to ensure fill is below cdf\n",
    "ax.fill_between(x[x<x_20], norm_pdf[x<x_20], alpha=0.4, zorder=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. Sampling random values from the probability distribution\n",
    "\n",
    "We may also want to run an experiment by generating random values from the normal distribution. Recall that in Unit 2, we created random values from a flat probability distribution using `numpy.random`. We can use the normal probability distribution object created in the cell above, and generate an array of random values from the distribution using the function `norm.rvs(size=n)` (random values). This function takes a parameter named `size` which is the length of the array that will be generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_random = stats.norm.rvs(size=200, loc=10, scale=0.6)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(norm_random, density=True)\n",
    "ax.plot(x, norm_pdf, label='pdf')\n",
    "\n",
    "ax.set_ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Constructing control charts for the results\n",
    "\n",
    "### 5.3.1. Plotting a run chart of the measurements\n",
    "\n",
    "We'll return now to our previous example with the apparently bimodal distribution of polymer modulus values. One initial step in this analysis may be to plot out the measurements in time-series order. Below, we very simply plot the individual values of the modulus, using `numpy.arange()` to create a set of index values for the x coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(modulus))\n",
    "\n",
    "# plot modulus values in the order the measurements were made\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, modulus, marker='x')\n",
    "\n",
    "ax.set_xlabel('Index of Data Point')\n",
    "ax.set_ylabel('Modulus (MPa)')\n",
    "ax.set_title('Individual Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting, because it looks like the first 30 points have a different range than the last 30 points. This simple run chart, containing just the individual values, exhibits considerable variation. A common technique in statistical process control (SPC) is the $\\bar{x}$ (\"x-bar\") chart, which plots averages of a fixed number of replicate points. The use of control charts such as these is common for industrial engineers and practicioners of the \"$6\\sigma$\" methodology. Engineering courses such as ISyE 512 dive into the detailed theory behind different forms of control charts and potential pitfalls.\n",
    "\n",
    "We can average consecutive values in an array by reshaping the 1d array to a 2d matrix which contains the number of columns that we would like to average together. Look at the results to the following code to check your understanding of what's happening with this step. These sample means can then be plotted as a run chart. Variability is decreased by averaging multiple values together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the array into a 2d matrix with 3 columns\n",
    "# -1 rows means that the number of rows will be automatically determined\n",
    "modulus_grouped = modulus.reshape((-1, 3))\n",
    "\n",
    "# create a new 1d array by averaging across the columns (axis=1)\n",
    "x_bar = modulus_grouped.mean(axis=1)\n",
    "\n",
    "# plot the sample mean values\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(np.arange(len(x_bar)), x_bar, marker='x')\n",
    "\n",
    "ax.set_xticks(np.arange(len(x_bar)))\n",
    "\n",
    "ax.set_xlabel('Index of Data Point')\n",
    "ax.set_ylabel('Modulus (MPa)')\n",
    "ax.set_title(r'$\\bar{x}$ Chart')\n",
    "\n",
    "# display the first 9 measurements, groupings, and the mean values to check understanding\n",
    "modulus[:9], modulus_grouped[:3, :], x_bar[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Central Limit Theorem\n",
    "\n",
    "This result, that the variability decreases as we average measurements, makes intuitive sense and is grounded in the Central Limit Theorem.\n",
    "\n",
    "Consider a set of laboratory measurements. We already know how to calculate the mean and standard deviation of the data. What do we do if there is a high degree of variability in the data? Intuitively, we might take multiple measurements, and average them together, to reduce noise in the data. \n",
    "\n",
    "The [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) describes what happens if we randomly draw $n$ samples from population with a mean, $\\mu$, and standard deviation, $\\sigma$. We can average the samples to get the variable $\\bar{x}_n$. The standard deviation of the averages will decrease as $n$ increases:\n",
    "\n",
    "$$\\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "So, as we average more samples, we reduce variability and improve the power of our measurement. An illustration of the effect of averaging is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vals = 1200\n",
    "norm_random = stats.norm().rvs(size=total_vals)\n",
    "\n",
    "# plot the histogram, with the normal pdf\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim((-3.5, 3.5))\n",
    "\n",
    "x = np.linspace(-3.5, 3.5, 100)\n",
    "\n",
    "# number of measurements to average together\n",
    "n_to_average = [1, 5, 10, 50]\n",
    "\n",
    "for n in n_to_average:\n",
    "\n",
    "    # reshape the 1d array into a 2d matrix (number of cols = n)\n",
    "    norm_reshape = norm_random.reshape((int(total_vals/n), n))\n",
    "\n",
    "    # take the mean across the replicate measurements in each row\n",
    "    norm_means = norm_reshape.mean(axis=1)\n",
    "\n",
    "    # add a histogram of the averaged values to the axis\n",
    "    counts, bins, patches = ax.hist(norm_means, density=True, alpha=0.5, \n",
    "                                    label=f'$\\\\bar{{x}}_{{{n}}}$')\n",
    "    \n",
    "    # get the color to match the normal pdf to the histogram\n",
    "    color = patches[0].get_facecolor()\n",
    "\n",
    "    # add the standard normal pdf to the axis\n",
    "    ax.plot(x, stats.norm(loc=0, scale=1/np.sqrt(n)).pdf(x), c=color, alpha=1)\n",
    "\n",
    "ax.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. Control limits\n",
    "\n",
    "Now that we understand how averaging values together will affect the variance of our data, we can create *control limits* to bracket the expected values for the measured parameter. Often, control limits are defined in terms of the range in standard deviations, e.g. \"$3\\sigma$\" or \"$6\\sigma$\". The choice of range will determine what types of error are more likely. Narrow limits will increase the likelihood that a point is labeled \"out of control\" when it is actually representative of the population. A typical value for process monitoring may be a $3\\sigma$ limit, where the control limits are given by $\\mu \\pm 1.5\\sigma$.\n",
    "\n",
    "Wider limits of $6\\sigma$ are used for the example below, and it is clear that this chart indicates that the sample means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean and stdev based on the initial 30 points\n",
    "mean_first30 = modulus[:30].mean()\n",
    "std_first30 = modulus[:30].std(ddof=1)\n",
    "\n",
    "# calculate the range for the control limits (6-sigma)\n",
    "control_range = 6 * std_first30 / np.sqrt(3)\n",
    "\n",
    "# plot the x-bar values\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(np.arange(len(x_bar)), x_bar, marker='x')\n",
    "\n",
    "ax.set_xticks(np.arange(len(x_bar)))\n",
    "ax.set_xlabel('Index of Data Point')\n",
    "ax.set_ylabel('Modulus (MPa)')\n",
    "ax.set_title(r'$\\bar{x}$ Chart')\n",
    "\n",
    "# plot the mean and control limits\n",
    "ax.axhline(mean_first30 + 0.5*control_range, ls='--')\n",
    "ax.text(0, mean_first30 + 0.5*control_range + 20, \"UCL\")\n",
    "\n",
    "ax.axhline(mean_first30, ls='--', color='gray', alpha=0.5)\n",
    "\n",
    "ax.axhline(mean_first30 - 0.5*control_range, ls='--')\n",
    "ax.text(0, mean_first30 - 0.5*control_range + 20, \"LCL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common partner to the $\\bar{x}$ chart is the $R$ chart, which displays the range (max-min) of each set of averaged values. If the range value is out of control, this indicates that variability amongst the samples is changing. The range is also calculated based on the reshaped array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = modulus_grouped.max(axis=1) - modulus_grouped.min(axis=1)\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control limits for the $R$ chart are a little different. You'll typically go to a table to get the values of several constants, which will depend on the number of measurements that are being average. Assuming you know (or have an estimate for) the standard deviation, the following formulas apply:\n",
    "\n",
    "$$LCL=D_1\\sigma$$\n",
    "$$CL=d_1\\sigma$$\n",
    "$$UCL=D_2\\sigma$$\n",
    "\n",
    "The values can be found in a statistical quality control book such as [**Introduction to Statistical Quality Control**]() by Douglas Montgomery. In this example, we do not observe any out of control points in the $R$ chart that would indicate a change in variability over the course of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate centerline (CL) and upper/lower control limits\n",
    "R_CL = 1.693*std_first30\n",
    "R_UCL = 4.358*std_first30\n",
    "R_LCL = 0\n",
    "\n",
    "# plot R chart\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(np.arange(len(R)), R, marker='x')\n",
    "\n",
    "ax.set_xticks(np.arange(len(x_bar)))\n",
    "\n",
    "ax.set_xlabel('Index of Data Point')\n",
    "ax.set_ylabel('Modulus (MPa)')\n",
    "ax.set_title(r'$R$ Chart')\n",
    "\n",
    "# plot the centerline and control limits\n",
    "ax.axhline(R_UCL, ls='--')\n",
    "ax.text(0, R_UCL + 10, \"UCL\")\n",
    "\n",
    "ax.axhline(R_CL, ls='--', color='gray', alpha=0.5)\n",
    "\n",
    "ax.axhline(R_LCL, ls='--')\n",
    "ax.text(0, R_LCL + 10, \"LCL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Testing 2 separate distributions\n",
    "\n",
    "Based on what we learned in the previous section, we may want to conduct a statistical test to determine if these two sets of data are from the same distribution. \n",
    "\n",
    "For clarity as we move through this exercise, we'll split the modulus array into two pieces labels `mod_A` and `mod_B`. This isn't necessary, but it will clean up the code and make it easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# split the modulus array into two separate arrays\n",
    "# mod_A is the first 30 values, mod_B is the last 30\n",
    "mod_A = modulus[:30]\n",
    "mod_B = modulus[30:]\n",
    "\n",
    "# plot the two distributions, with the overlaid normal distribution\n",
    "fig, ax = plt.subplots() \n",
    "\n",
    "# plotting the first and second half of the data separately\n",
    "# here, setting the transparency (alpha) to 40% helps visualize overlap\n",
    "ax.hist(mod_A, bins=bin_edges, density=True, color='red', alpha=0.4, label='Modulus of A')\n",
    "ax.hist(mod_B, bins=bin_edges, density=True, color='blue', alpha=0.4, label='Modulus of B')\n",
    "\n",
    "ax.set_xlabel('Tensile Modulus (MPa)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "\n",
    "# overlay the standard normal distributions for the two sets\n",
    "\n",
    "# first, create an array of x-values to calculate the distributions over\n",
    "x = np.linspace(modulus.min()-modulus.std(), modulus.max()+modulus.std(), 100)\n",
    "\n",
    "pdf_A = stats.norm.pdf(x, loc=mod_A.mean(), scale=mod_A.std(ddof=1))\n",
    "pdf_B = stats.norm.pdf(x, loc=mod_B.mean(), scale=mod_B.std(ddof=1))\n",
    "\n",
    "ax.plot(x, pdf_A, color='red')\n",
    "ax.plot(x, pdf_B, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These data sets visually appear to be different, but is there statistical evidence to support that assumption? We can use the *Student's t-test*. When applying a statistical hypothesis test, we must start with a *null hypothesis*, $H_0$, generally that the mean of these two populations is the same: $\\mu_A=\\mu_B$. The *alternative hypothesis*, $H_1$, is that the population means are not the same: $\\mu_A \\ne \\mu_B$. There are several versions of the *t-test* statistic, depending on whether the number of values is the same in both sample data sets ($n_A=n_B$) and whether the sample variances can be assumed as equal ($\\sigma^2_A=\\sigma^2_B$). \n",
    "\n",
    "For equal variances, $\\sigma^2_A=\\sigma^2_B$, a pooled estimate of the variance is calculated from the two sample variances. \n",
    "\n",
    "$$s^2_p=\\frac{\\left(n_A-1\\right)s^2_A + \\left(n_B-1\\right)s^2_B}{n_A+n_B-2}$$\n",
    "\n",
    "Note the use of $s^2$ to indicate the *sample* standard deviation, which is calculated based on $n-1$ degrees of freedom. To calculate this value using the `ndarray.var()` function, you must set the parameter `ddof=1`. Otherwise you'll calculate the *population* variance, based on $n$ degrees of freedom, and introduce a slight error.\n",
    "\n",
    "The t-statistic for the null hypothesis is calculated as \n",
    "\n",
    "$$t_0=\\frac{\\bar{x}_A-\\bar{x}_B}{s_p\\sqrt{\\frac{1}{n_A}-\\frac{1}{n_B}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_A = len(mod_A)\n",
    "n_B = len(mod_B)\n",
    "\n",
    "sp = np.sqrt(((n_A-1)*mod_A.var(ddof=1) + (n_B-1)*mod_B.var(ddof=1))/(n_A+n_B-2))\n",
    "t0 = (mod_A.mean()-mod_B.mean())/(sp*np.sqrt(1/n_A+1/n_B))\n",
    "\n",
    "t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there is a `scipy.stats` function, `stats.ttest_ind()`, that takes care of these calculations for you. There is a parameter, `equal_var`, that can be set as `True` or `False` to confirm your assumption about equal variances for the two sample sets. In addition to the t-statistic, which matches our previous calculation, this function provides the *p-value* for the result. This is the probability of observing these data sets, given the null hypothesis being true. When the p-value is low, less than 0.05 or 0.01, we reject the null hypothesis as being statistically unlikely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(mod_A, mod_B, equal_var=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.t.isf(0.025, df=29, loc=mod_A.mean(), scale=mod_A.std(ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One caution with hypothesis testing is that, while we may find that a result is *statistically* significant, it may or may not be *practically* significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.96*15/np.sqrt(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.interval(0.95, scale=15/np.sqrt(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_distributions(ax, std):\n",
    "\n",
    "    x = np.linspace(50, 150, 100)\n",
    "\n",
    "    xbar_1 = 95\n",
    "    xbar_2 = 105\n",
    "\n",
    "    values_1 = stats.norm(loc=xbar_1, scale=std).rvs(30)\n",
    "    values_2 = stats.norm(loc=xbar_2, scale=std).rvs(30)\n",
    "\n",
    "    ci_1 = stats.t.interval(0.95, scale=values_1.std(ddof=1)/np.sqrt(30), df=30)\n",
    "    ci_2 = stats.t.interval(0.95, scale=values_1.std(ddof=1)/np.sqrt(30), df=30)\n",
    "\n",
    "    ax.hist(values_1, density=True, alpha=0.4)\n",
    "    ax.hist(values_2, density=True, alpha=0.4)\n",
    "\n",
    "    ax.errorbar(values_1.mean(), 0.09, xerr=ci_1[1], capsize=6)\n",
    "    ax.errorbar(values_2.mean(), 0.09, xerr=ci_2[1], capsize=6)\n",
    "\n",
    "    print(stats.ttest_ind(values_1, values_2))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, figsize=(4, 6), sharex=True, sharey=True)\n",
    "ax[0].set_ylim((0, 0.1))\n",
    "\n",
    "example_distributions(ax[0], 5)\n",
    "example_distributions(ax[1], 10)\n",
    "example_distributions(ax[2], 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Next Steps:\n",
    "\n",
    "1. Complete the [Unit 5 Problems](./unit05-solutions.ipynb) to test your understanding\n",
    "2. Advance to [Unit 6](../06-regression-classification/unit06-lesson.ipynb) when you're ready for the next step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2af3520cf2d099315c509e23b0f679c1508a4671732b5e09eee0e9586de4a344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
