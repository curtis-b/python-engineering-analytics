{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 4: The `pandas` DataFrame\n",
    "--------------------------------\n",
    "\n",
    "In Unit 2, we introduced the `DataFrame` for loading data and computing descriptive statistics. Now, we'll take this a step further and demonstrate the use of pivot tables using `pandas`. If you've used pivot tables in a spreadsheet program, the concept is similar: in a pivot table, we summarize data based on one or more categorical variables. This can be a fast way to compute averages or other descriptive statistics for each category in a dataset. When we perform this data processing step in Python, we can then use the pivot table data for plotting in `matplotlib` or save the results to a file.\n",
    "\n",
    "**After completing this unit, you should be able to:**\n",
    "\n",
    "- Select and filter a `pandas.DataFrame`\n",
    "- Create new columns by applying functions to existing data\n",
    "- Create and save a pivot table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Selecting and filtering data\n",
    "\n",
    "In Unit 2 we introduced the `pandas.DataFrame` object, including how to load data, and basic selections by column and index. We'll start by reviewing the layout of the `DataFrame` in more depth. We can think about the data frame in terms of the image below, where the data is organized in terms of an *index* (the rows) and *columns*. By default, the index will just be a row number, but it can also be customized as we'll see later with pivot tables.\n",
    "\n",
    "![DataFrame layout](../../assets/DataFrame.svg)\n",
    "\n",
    "We'll revisit the example from Unit 2 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read a .csv file into a DataFrame variable named df\n",
    "fid_df = pd.read_csv('../../data/pcr-polyethylene_gc-fid.csv')\n",
    "\n",
    "# display the top n rows in the DataFrame named df (defaults to n=5 if blank)\n",
    "fid_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the index by using the `DataFrame.index` property. In this example the default index is a range. Similar to the `numpy.arange()` that we used to create an array in Unit 2, this range is defined with start, stop and step values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 ways that we can select rows from the `DataFrame` object: \n",
    "\n",
    "1. Select by the data *value*\n",
    "2. Select by the index *number* (`iloc`)\n",
    "3. Select by the index *value* (`loc`)\n",
    "\n",
    "To begin, let's select rows based on the value of the time. This data represents the flame ionization detector (FID) signal from a gas chromatograph. This is a time series (with units of minutes) and the FID sensor data. We might know that the true data begins at 6 minutes, so the dataset should be filtered to times greater than 6. \n",
    "\n",
    "The notatation for this filter may look strange, so let's work our way from the inside out. \n",
    "\n",
    "The first piece of this code \n",
    "\n",
    "```\n",
    "fid_df['minutes']\n",
    "```\n",
    "\n",
    "selects the minutes column, which contains the data that we want to use for our filter. We learned about this in Unit 2. Now, we take the data in that column and run the test to see if it is greater than our chosen start time, 6 minutes.\n",
    "\n",
    "```\n",
    "fid_df['minutes'] > 6\n",
    "```\n",
    "\n",
    "This comparison returns a series of the same shape as `fid_df['minutes']`, with a value of `True` if this time is greater than 6 or `False` if the time is less than or equal to 6. Note that we could have used any other `True`/`False` test, including `<`, `<=`, `>=`, `==` (equal), `!=` (not equal). Finally, this series of `True`/`False` values is passed to the `DataFrame`.\n",
    "\n",
    "```\n",
    "fid_df[fid_df['minutes'] > 6]\n",
    "```\n",
    "\n",
    "Any rows where the series returned by the test `fid_df['minutes'] > 6` has returned `True` are part of the selected rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df[fid_df['minutes'] > 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the filter is applied you should find that we now have 46983 rows, compared to 53184 in the original `DataFrame`. These rows have not been deleted! They still exist in the original `fid_df` object. Now, we might want to apply multiple filters using boolean and/or logic. This can be accomplished as well.\n",
    "\n",
    "In the example below, we are looking for values where the time is both greater than 6 *and* less than 7 minutes. The first thing to note in the example below is that we have now enclosed each condition in parentheses to separate it. This is required in this case, because the order of operations is not well-defined for this complicated set of instructions. Second, we are using the bitwise *and* operator `&` to indicate that both conditions must be `True` for the result to be `True`. There are additional operators for bitwise *or* `|` and bitwise exclusive or (*xor*) `^`. The *xor* operation means that only one of the conditions may be `True` (one, but not both).\n",
    "\n",
    "Truth table:\n",
    "\n",
    "| Value 1 | Operator | Value 2 | Result |\n",
    "|---------|----------|---------|--------|\n",
    "| `True` | `&` | `True` | `True` |\n",
    "| `True` | `&` | `False` | `False` |\n",
    "| `False` | `&` | `True` | `False` |\n",
    "| `False` | `&` | `False` | `False` |\n",
    "| `True` | `\\|` | `True` | `True` |\n",
    "| `True` | `\\|` | `False` | `True` |\n",
    "| `False` | `\\|` | `True` | `True` |\n",
    "| `False` | `\\|` | `False` | `False` |\n",
    "| `True` | `^` | `True` | `False` |\n",
    "| `True` | `^` | `False` | `True` |\n",
    "| `False` | `^` | `True` | `True` |\n",
    "| `False` | `^` | `False` | `False` |\n",
    "\n",
    "Experiment with the code below to apply different filters to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df[(fid_df['minutes'] > 6) & (fid_df['minutes'] < 7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two ways of selecting rows deal with the row index number and row index value. In our current example, these are interchangeable because the index is a series of integers that are identical to the row number. So, the `loc` and `iloc` properties will work the same. To make it more clear what the difference is between these two, we will reload our data. This time, we set the row index to be the *minutes* column from our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df_timeindexed = pd.read_csv('../../data/pcr-polyethylene_gc-fid.csv', index_col='minutes')\n",
    "fid_df_timeindexed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we look at the index, we see that it is no longer a range but an array of floating-point (real) numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df_timeindexed.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select rows based on index number (even though the index values are now the time in minutes), we use the `DataFrame.iloc` property and our array slicing notation. This example selects the first 5 rows from the `DataFrame`, just like using the `DataFrame.head(5)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df_timeindexed.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to select individual rows is to provide a list (or array) of row index numbers to `iloc`. Note the double square brackets - the inner brackets indicate that we are creating a list with the values `1`, `3` and `5`. This list is being passed to the `iloc` accessor to select the individual rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df_timeindexed.iloc[[1, 5, 10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we know how to create a sequence of values using `numpy.arange`. For instance, we could select every even-numbered value using the following sequence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fid_df_timeindexed.iloc[np.arange(0, len(fid_df_timeindexed), 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of what the index looks like, we can always use an integer to select rows, using the `iloc` accessor. In contrast, the `loc` accessor slices based on the *value* in the index column, which may not be an integer value. To see how this works, let's revisit our previous example of selecting based on time ranges. Because the index is now defined as the time values, we can use the simple slicing notation to select all points where the time is greater than 6 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df_timeindexed.loc[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can get the time interval between 6 and 7 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df_timeindexed.loc[6:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This slicing notation is very compact to write and can be convenient. However, it does lack some of the flexibility of the column-based filters that we introduced earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Applying a function to create a new column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new column in a `DataFrame` can be be easily done. Perhaps you would prefer to have the elution time from our GC dataset in units of seconds, instead of minutes. We can simply select the `minutes` column as before, multiply the entire column by 60, and assign the result to a new column called `seconds`. The `pandas` code will take care of creating this new column for us, even though it didn't exist previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df['seconds'] = fid_df['minutes'] * 60\n",
    "fid_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can easily apply operations such as differencing or moving-window functions. *Differencing* is a common technique in time series analysis to de-trend data. In this operation, for each row in the data series, the previous value is subtracted so that only the difference between the two sequencial points remains. This can also be used to calculate a numerical derivative from a data set. In the example below.\n",
    "\n",
    "$$\\frac{\\Delta signal}{\\Delta time}=\\frac{\\texttt{fid\\_df[`fid'].diff()}}{\\texttt{fid\\_df[`seconds'].diff()}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# calculate the first derivative (change in FID signal) / (change in time)\n",
    "fid_df['1st_derivative'] = fid_df['fid'].diff() / fid_df['seconds'].diff()\n",
    "\n",
    "# plot the FID signal and its 1st derivative\n",
    "# the \\n in the label is the new-line character to split the label into 2 lines\n",
    "fig, ax = plt.subplots(nrows=2, sharex=True)\n",
    "\n",
    "ax[0].plot(fid_df['seconds'], fid_df['fid'])\n",
    "ax[0].set_ylabel('Original\\nFID Signal')\n",
    "\n",
    "ax[1].plot(fid_df['seconds'], fid_df['1st_derivative'])\n",
    "ax[1].set_xlabel('Seconds')\n",
    "ax[1].set_ylabel('1st Derivative\\nof FID Signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important tool in analyzing time-series data is the moving-average operation. This can reduce noise in the data, smoothing the data series. Sometimes, a smoothing operation (either a moving average, or a more sophisticated signal filter) will be applied to the data prior to computing a derivative. This can make the resulting derivate much less noisy.\n",
    "\n",
    "The `Series.rolling(n)` function creates a moving window of $n$ rows. We apply a function, such as `Series.mean()` to this window to average the datapoints within the window. By default, this is a trailing window. In the example below, we see that the first available value is in row index 4 (the 5th row), and this contains the `Series.mean()` of this row together with the previous 4 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df['fid'].rolling(5).mean().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the size of our window increases, the trailing moving average may appear to be out of sync with our data. We may want to shift the data to center our average in the window. We can do this with the `Series.shift(n)` function, which moves the data relative to its index value. If the value of $n$ is positive, we shift the data right (to higher index values). If the value of $n$ is negative, we move the data left (to lower index values). Below, the rolling mean is shifted left by two positions to center the moving average in the window. This creates 2 blank `NaN` (Not a Number) values at the end of the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df['fid'].rolling(5).mean().shift(-2).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df['fid'].rolling(5).mean().shift(-2).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use what we've learned about the moving average to create a new column in our `DataFrame` with the signal smoothed out. This operation removes the high peaks from the data, which may or may not be appropriate for your analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df['rolling_mean'] = fid_df['fid'].rolling(51).mean().shift(-25)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fid_df['seconds'], fid_df['fid'], label='Original FID Signal')\n",
    "ax.plot(fid_df['seconds'], fid_df['rolling_mean'], label='Rolling Mean')\n",
    "\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll save the results of our analysis to a comma-separated (csv) text file. By adding the `index=False` parameter, we will exclude the integer row index from our output file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_df.to_csv('../../output/unit4_fid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Creating and saving a pivot table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Long versus wide data sets\n",
    "\n",
    "Pivot tables are most useful on long-form datasets, which have a minimum number of columns. Consider the dataset below, which contains measurements of different properties of polymer films. In a *wide* form, we might build the data with one column per property, and then one row per test value. In *long* form there is a single column containing all of the measured values. To clarify what the measured value represents, we add a categorical value as a new column to define specifically what property was measured. This data structure can be more flexible to adding new properties in the future, especially if the data is being stored in a relational database. Rather than adding new columns each time a test is added, we can just add new rows with the new `Property` value. This form also allows for different numbers of replicate measurements for each film property, while not introducing any blank spaces in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_df = pd.read_excel('../../data/film_testing.xlsx', sheet_name='physical_properties')\n",
    "film_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Creating the pivot table\n",
    "\n",
    "We can use a pivot table to summarize these results, averaging over the individual polymer grades within each polymer family. Look at the parameters in the `DataFrame.pivot_table()` function below. The column named `Measurement` is used to calculate the values in the pivot table - this is where the numerical data is stored. The column named `FilmType` is used to generate the row index. There will be a row for each unique value from this column. The column named `Property` (what we are measuring) will become a column heading in the pivot table, with one column for each unique value of a property. Finally, we can define the `aggfunc` parameter to say what mathematical operation we wish to apply to the data values. Optional values are: `mean`, `max`, `min`, `count`, `stdev`. \n",
    "\n",
    "The output of the `DataFrame.pivot_table()` function is another `DataFrame`, with the row indices and column names as we have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_pt = film_df.pivot_table(values='Measurement', index='FilmType', columns='Property', aggfunc='mean')\n",
    "film_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a `DataFrame`, our same rules for indexing and column selection still apply. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_pt['Tensile Modulus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_pt['Tensile Modulus'].loc['BOPP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot tables can get more detailed (and complicated) by using lists of multiple columns for the `index` or `columns` parameters. Look at the example below and see how these values are nested. The rows are first clustered by the `FilmType` value, and then by `FilmID`. Also have two levels for the test method, and the direction in which the film is tested (**m**achine **d**irection / **t**ransverse **d**irection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_pt2 = film_df.pivot_table(values='Measurement', index=['FilmType', 'FilmID'], columns=['Property', 'Direction'], aggfunc='mean')\n",
    "film_pt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting rows and columns which are nested can be easily done, by separating the individual selections by comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_pt2['Tensile Modulus', 'MD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One additional step that we often take is to plot the results of a pivot table. In the example below, we plot the average modulus for high-density polyethylene (HDPE) and polypropylene (PP) polymers. The values from our pivot table index are used to label the bars in this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# create a range of values for x, since our index is not numeric\n",
    "x = np.arange(len(film_pt))\n",
    "\n",
    "# plot the values, applying the index values as labels\n",
    "ax.bar(x, film_pt['Tensile Modulus'], tick_label=film_pt.index)\n",
    "\n",
    "# label the numerical axis\n",
    "ax.set_ylabel('Tensile Modulus (MPa)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3. Saving the results\n",
    "\n",
    "Again, the pivot table is just another `DataFrame`, so we can save the results. In the empty cell below, practice saving the `film_pt2` pivot table to a file in the `output` directory. Open the file to verify that it saved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Next Steps:\n",
    "\n",
    "1. Complete the [Unit 4 Problems](./unit04-solutions.ipynb) to test your understanding\n",
    "2. Advance to [Unit 5](../05-statistics/unit05-lesson.ipynb) when you're ready for the next step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2857618fbf4d4bb3d9a88a0331cc9f5539aea2b45b45163c4b048952b7884495"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
